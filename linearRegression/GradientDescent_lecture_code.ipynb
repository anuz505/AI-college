{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvx6Ar4c1pSe"
   },
   "source": [
    "# Gradient descent for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlKB5q4D1y10"
   },
   "source": [
    "Now that you have learned how Gradient Descent works, let's use it to find the parameters of Linear Regression. Let's fit the linear regression on the same [Advertisesment Dataset](https://www.statlearning.com/s/Advertising.csv), but this time using Gradient Descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReVUHCNHiGWL"
   },
   "source": [
    "Let's import the necessary libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AcqV3VsCGXpY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnBw7u4u_L_O"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 1795,
     "status": "ok",
     "timestamp": 1620133587058,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "Fx3xywro_Pv3",
    "outputId": "2aebed00-23a5-46a8-bab0-9637c6469715"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "1  230.1   37.8       69.2   22.1\n",
       "2   44.5   39.3       45.1   10.4\n",
       "3   17.2   45.9       69.3    9.3\n",
       "4  151.5   41.3       58.5   18.5\n",
       "5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"https://www.statlearning.com/s/Advertising.csv\" \n",
    "     \n",
    "\n",
    "# Read the CSV data from the link\n",
    "data_df = pd.read_csv(data_path,index_col=0)\n",
    "\n",
    "# Print out first 5 samples from the DataFrame\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce49736IcehF"
   },
   "source": [
    "This is a multiple Linear Regression problem with three independent variables: TV , radio and newspaper and one dependent variable: sales. There are 200 samples in the dataset *ie.* $n = 200$\n",
    "\n",
    "This multiple linear regression problem can be represented in matrix form as:\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = \\mathbf{X} \\boldsymbol{\\beta}$$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\hat{y_1} \\\\ \n",
    "\\hat{y_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y_{200}}\n",
    "\\end{bmatrix} =   \\begin{bmatrix}\n",
    "  1 & x_{1\\ 1} & x_{1\\ 2} & x_{1\\ 3} \\\\\n",
    "  1 & x_{2\\ 1} & x_{2\\ 2} & x_{2\\ 3} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  1 & x_{200\\ 1} & x_{200\\ 2} & x_{200\\ 3}\n",
    " \\end{bmatrix} \\times \\begin{bmatrix}\n",
    "\\beta_0 \\\\ \n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\beta_3\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "The predicted output for the samples can be computed as:\n",
    "\\begin{align*}\\hat{y_1} &= \\beta_0x_{1\\ 0}+ \\beta_1x_{1\\ 1} + \\beta_2x_{1\\ 2} + \\beta_3 x_{1\\ 3}\\\\\n",
    "\\hat{y_2} &= \\beta_0x_{2\\ 0}+ \\beta_2x_{2\\ 1} + \\beta_2x_{2\\ 2} + \\beta_3 x_{2\\ 3}\\\\\n",
    "\\hat{y_3} &= \\beta_0x_{3\\ 0}+ \\beta_1x_{3\\ 1} + \\beta_2x_{3\\ 2} + \\beta_3 x_{3\\ 3}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "\\hat{y_{200}} &= \\beta_0x_{200\\ 0}+ \\beta_1x_{200\\ 1} + \\beta_2x_{200\\ 2} + \\beta_3 x_{200\\ 3}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Generalizing, for any $i^{th}$ sample, predicted output can be computed as:\n",
    "\n",
    "$$\\hat{y_i} = \\beta_0x_{i\\ 0}+ \\beta_1x_{i\\ 1} + \\beta_2x_{i\\ 2} + \\beta_3 x_{i\\ 3}$$\n",
    " where for all $i$ = $1$ to $n$, $x_{i0} =1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5BWRU-N_Uz-"
   },
   "source": [
    "The following block of codes creates a matrix $\\mathbf{X}$ containing the features of all the samples and a vector $\\mathbf{y}$ containing their corresponding outputs. It also assigns the number of samples to $n$ and the number of features to $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qbsl4vbTFk3i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1. , 230.1,  37.8,  69.2],\n",
       "       [  1. ,  44.5,  39.3,  45.1],\n",
       "       [  1. ,  17.2,  45.9,  69.3],\n",
       "       [  1. , 151.5,  41.3,  58.5],\n",
       "       [  1. , 180.8,  10.8,  58.4],\n",
       "       [  1. ,   8.7,  48.9,  75. ],\n",
       "       [  1. ,  57.5,  32.8,  23.5],\n",
       "       [  1. , 120.2,  19.6,  11.6],\n",
       "       [  1. ,   8.6,   2.1,   1. ],\n",
       "       [  1. , 199.8,   2.6,  21.2],\n",
       "       [  1. ,  66.1,   5.8,  24.2],\n",
       "       [  1. , 214.7,  24. ,   4. ],\n",
       "       [  1. ,  23.8,  35.1,  65.9],\n",
       "       [  1. ,  97.5,   7.6,   7.2],\n",
       "       [  1. , 204.1,  32.9,  46. ],\n",
       "       [  1. , 195.4,  47.7,  52.9],\n",
       "       [  1. ,  67.8,  36.6, 114. ],\n",
       "       [  1. , 281.4,  39.6,  55.8],\n",
       "       [  1. ,  69.2,  20.5,  18.3],\n",
       "       [  1. , 147.3,  23.9,  19.1],\n",
       "       [  1. , 218.4,  27.7,  53.4],\n",
       "       [  1. , 237.4,   5.1,  23.5],\n",
       "       [  1. ,  13.2,  15.9,  49.6],\n",
       "       [  1. , 228.3,  16.9,  26.2],\n",
       "       [  1. ,  62.3,  12.6,  18.3],\n",
       "       [  1. , 262.9,   3.5,  19.5],\n",
       "       [  1. , 142.9,  29.3,  12.6],\n",
       "       [  1. , 240.1,  16.7,  22.9],\n",
       "       [  1. , 248.8,  27.1,  22.9],\n",
       "       [  1. ,  70.6,  16. ,  40.8],\n",
       "       [  1. , 292.9,  28.3,  43.2],\n",
       "       [  1. , 112.9,  17.4,  38.6],\n",
       "       [  1. ,  97.2,   1.5,  30. ],\n",
       "       [  1. , 265.6,  20. ,   0.3],\n",
       "       [  1. ,  95.7,   1.4,   7.4],\n",
       "       [  1. , 290.7,   4.1,   8.5],\n",
       "       [  1. , 266.9,  43.8,   5. ],\n",
       "       [  1. ,  74.7,  49.4,  45.7],\n",
       "       [  1. ,  43.1,  26.7,  35.1],\n",
       "       [  1. , 228. ,  37.7,  32. ],\n",
       "       [  1. , 202.5,  22.3,  31.6],\n",
       "       [  1. , 177. ,  33.4,  38.7],\n",
       "       [  1. , 293.6,  27.7,   1.8],\n",
       "       [  1. , 206.9,   8.4,  26.4],\n",
       "       [  1. ,  25.1,  25.7,  43.3],\n",
       "       [  1. , 175.1,  22.5,  31.5],\n",
       "       [  1. ,  89.7,   9.9,  35.7],\n",
       "       [  1. , 239.9,  41.5,  18.5],\n",
       "       [  1. , 227.2,  15.8,  49.9],\n",
       "       [  1. ,  66.9,  11.7,  36.8],\n",
       "       [  1. , 199.8,   3.1,  34.6],\n",
       "       [  1. , 100.4,   9.6,   3.6],\n",
       "       [  1. , 216.4,  41.7,  39.6],\n",
       "       [  1. , 182.6,  46.2,  58.7],\n",
       "       [  1. , 262.7,  28.8,  15.9],\n",
       "       [  1. , 198.9,  49.4,  60. ],\n",
       "       [  1. ,   7.3,  28.1,  41.4],\n",
       "       [  1. , 136.2,  19.2,  16.6],\n",
       "       [  1. , 210.8,  49.6,  37.7],\n",
       "       [  1. , 210.7,  29.5,   9.3],\n",
       "       [  1. ,  53.5,   2. ,  21.4],\n",
       "       [  1. , 261.3,  42.7,  54.7],\n",
       "       [  1. , 239.3,  15.5,  27.3],\n",
       "       [  1. , 102.7,  29.6,   8.4],\n",
       "       [  1. , 131.1,  42.8,  28.9],\n",
       "       [  1. ,  69. ,   9.3,   0.9],\n",
       "       [  1. ,  31.5,  24.6,   2.2],\n",
       "       [  1. , 139.3,  14.5,  10.2],\n",
       "       [  1. , 237.4,  27.5,  11. ],\n",
       "       [  1. , 216.8,  43.9,  27.2],\n",
       "       [  1. , 199.1,  30.6,  38.7],\n",
       "       [  1. , 109.8,  14.3,  31.7],\n",
       "       [  1. ,  26.8,  33. ,  19.3],\n",
       "       [  1. , 129.4,   5.7,  31.3],\n",
       "       [  1. , 213.4,  24.6,  13.1],\n",
       "       [  1. ,  16.9,  43.7,  89.4],\n",
       "       [  1. ,  27.5,   1.6,  20.7],\n",
       "       [  1. , 120.5,  28.5,  14.2],\n",
       "       [  1. ,   5.4,  29.9,   9.4],\n",
       "       [  1. , 116. ,   7.7,  23.1],\n",
       "       [  1. ,  76.4,  26.7,  22.3],\n",
       "       [  1. , 239.8,   4.1,  36.9],\n",
       "       [  1. ,  75.3,  20.3,  32.5],\n",
       "       [  1. ,  68.4,  44.5,  35.6],\n",
       "       [  1. , 213.5,  43. ,  33.8],\n",
       "       [  1. , 193.2,  18.4,  65.7],\n",
       "       [  1. ,  76.3,  27.5,  16. ],\n",
       "       [  1. , 110.7,  40.6,  63.2],\n",
       "       [  1. ,  88.3,  25.5,  73.4],\n",
       "       [  1. , 109.8,  47.8,  51.4],\n",
       "       [  1. , 134.3,   4.9,   9.3],\n",
       "       [  1. ,  28.6,   1.5,  33. ],\n",
       "       [  1. , 217.7,  33.5,  59. ],\n",
       "       [  1. , 250.9,  36.5,  72.3],\n",
       "       [  1. , 107.4,  14. ,  10.9],\n",
       "       [  1. , 163.3,  31.6,  52.9],\n",
       "       [  1. , 197.6,   3.5,   5.9],\n",
       "       [  1. , 184.9,  21. ,  22. ],\n",
       "       [  1. , 289.7,  42.3,  51.2],\n",
       "       [  1. , 135.2,  41.7,  45.9],\n",
       "       [  1. , 222.4,   4.3,  49.8],\n",
       "       [  1. , 296.4,  36.3, 100.9],\n",
       "       [  1. , 280.2,  10.1,  21.4],\n",
       "       [  1. , 187.9,  17.2,  17.9],\n",
       "       [  1. , 238.2,  34.3,   5.3],\n",
       "       [  1. , 137.9,  46.4,  59. ],\n",
       "       [  1. ,  25. ,  11. ,  29.7],\n",
       "       [  1. ,  90.4,   0.3,  23.2],\n",
       "       [  1. ,  13.1,   0.4,  25.6],\n",
       "       [  1. , 255.4,  26.9,   5.5],\n",
       "       [  1. , 225.8,   8.2,  56.5],\n",
       "       [  1. , 241.7,  38. ,  23.2],\n",
       "       [  1. , 175.7,  15.4,   2.4],\n",
       "       [  1. , 209.6,  20.6,  10.7],\n",
       "       [  1. ,  78.2,  46.8,  34.5],\n",
       "       [  1. ,  75.1,  35. ,  52.7],\n",
       "       [  1. , 139.2,  14.3,  25.6],\n",
       "       [  1. ,  76.4,   0.8,  14.8],\n",
       "       [  1. , 125.7,  36.9,  79.2],\n",
       "       [  1. ,  19.4,  16. ,  22.3],\n",
       "       [  1. , 141.3,  26.8,  46.2],\n",
       "       [  1. ,  18.8,  21.7,  50.4],\n",
       "       [  1. , 224. ,   2.4,  15.6],\n",
       "       [  1. , 123.1,  34.6,  12.4],\n",
       "       [  1. , 229.5,  32.3,  74.2],\n",
       "       [  1. ,  87.2,  11.8,  25.9],\n",
       "       [  1. ,   7.8,  38.9,  50.6],\n",
       "       [  1. ,  80.2,   0. ,   9.2],\n",
       "       [  1. , 220.3,  49. ,   3.2],\n",
       "       [  1. ,  59.6,  12. ,  43.1],\n",
       "       [  1. ,   0.7,  39.6,   8.7],\n",
       "       [  1. , 265.2,   2.9,  43. ],\n",
       "       [  1. ,   8.4,  27.2,   2.1],\n",
       "       [  1. , 219.8,  33.5,  45.1],\n",
       "       [  1. ,  36.9,  38.6,  65.6],\n",
       "       [  1. ,  48.3,  47. ,   8.5],\n",
       "       [  1. ,  25.6,  39. ,   9.3],\n",
       "       [  1. , 273.7,  28.9,  59.7],\n",
       "       [  1. ,  43. ,  25.9,  20.5],\n",
       "       [  1. , 184.9,  43.9,   1.7],\n",
       "       [  1. ,  73.4,  17. ,  12.9],\n",
       "       [  1. , 193.7,  35.4,  75.6],\n",
       "       [  1. , 220.5,  33.2,  37.9],\n",
       "       [  1. , 104.6,   5.7,  34.4],\n",
       "       [  1. ,  96.2,  14.8,  38.9],\n",
       "       [  1. , 140.3,   1.9,   9. ],\n",
       "       [  1. , 240.1,   7.3,   8.7],\n",
       "       [  1. , 243.2,  49. ,  44.3],\n",
       "       [  1. ,  38. ,  40.3,  11.9],\n",
       "       [  1. ,  44.7,  25.8,  20.6],\n",
       "       [  1. , 280.7,  13.9,  37. ],\n",
       "       [  1. , 121. ,   8.4,  48.7],\n",
       "       [  1. , 197.6,  23.3,  14.2],\n",
       "       [  1. , 171.3,  39.7,  37.7],\n",
       "       [  1. , 187.8,  21.1,   9.5],\n",
       "       [  1. ,   4.1,  11.6,   5.7],\n",
       "       [  1. ,  93.9,  43.5,  50.5],\n",
       "       [  1. , 149.8,   1.3,  24.3],\n",
       "       [  1. ,  11.7,  36.9,  45.2],\n",
       "       [  1. , 131.7,  18.4,  34.6],\n",
       "       [  1. , 172.5,  18.1,  30.7],\n",
       "       [  1. ,  85.7,  35.8,  49.3],\n",
       "       [  1. , 188.4,  18.1,  25.6],\n",
       "       [  1. , 163.5,  36.8,   7.4],\n",
       "       [  1. , 117.2,  14.7,   5.4],\n",
       "       [  1. , 234.5,   3.4,  84.8],\n",
       "       [  1. ,  17.9,  37.6,  21.6],\n",
       "       [  1. , 206.8,   5.2,  19.4],\n",
       "       [  1. , 215.4,  23.6,  57.6],\n",
       "       [  1. , 284.3,  10.6,   6.4],\n",
       "       [  1. ,  50. ,  11.6,  18.4],\n",
       "       [  1. , 164.5,  20.9,  47.4],\n",
       "       [  1. ,  19.6,  20.1,  17. ],\n",
       "       [  1. , 168.4,   7.1,  12.8],\n",
       "       [  1. , 222.4,   3.4,  13.1],\n",
       "       [  1. , 276.9,  48.9,  41.8],\n",
       "       [  1. , 248.4,  30.2,  20.3],\n",
       "       [  1. , 170.2,   7.8,  35.2],\n",
       "       [  1. , 276.7,   2.3,  23.7],\n",
       "       [  1. , 165.6,  10. ,  17.6],\n",
       "       [  1. , 156.6,   2.6,   8.3],\n",
       "       [  1. , 218.5,   5.4,  27.4],\n",
       "       [  1. ,  56.2,   5.7,  29.7],\n",
       "       [  1. , 287.6,  43. ,  71.8],\n",
       "       [  1. , 253.8,  21.3,  30. ],\n",
       "       [  1. , 205. ,  45.1,  19.6],\n",
       "       [  1. , 139.5,   2.1,  26.6],\n",
       "       [  1. , 191.1,  28.7,  18.2],\n",
       "       [  1. , 286. ,  13.9,   3.7],\n",
       "       [  1. ,  18.7,  12.1,  23.4],\n",
       "       [  1. ,  39.5,  41.1,   5.8],\n",
       "       [  1. ,  75.5,  10.8,   6. ],\n",
       "       [  1. ,  17.2,   4.1,  31.6],\n",
       "       [  1. , 166.8,  42. ,   3.6],\n",
       "       [  1. , 149.7,  35.6,   6. ],\n",
       "       [  1. ,  38.2,   3.7,  13.8],\n",
       "       [  1. ,  94.2,   4.9,   8.1],\n",
       "       [  1. , 177. ,   9.3,   6.4],\n",
       "       [  1. , 283.6,  42. ,  66.2],\n",
       "       [  1. , 232.1,   8.6,   8.7]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature matrix\n",
    "X = data_df[['TV', 'radio', 'newspaper']].values\n",
    "\n",
    "# Adding the column of ones in X\n",
    "X = np.c_[np.ones((X.shape[0],1)),X]\n",
    "\n",
    "# Outputs\n",
    "y = data_df['sales'].values.reshape(-1,1)\n",
    "\n",
    "n = X.shape[0] # number of samples (rows)\n",
    "d = X.shape[1] # number of features (columns)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PucmdL0EtSaM"
   },
   "source": [
    "## Random Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nt6JrDuKidI"
   },
   "source": [
    "Let's initialize the values of parameters randomly. The function `initialize_beta` uses the [`numpy.random.randn`](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html) function to initialize the parameters using the random values sampled from **standard normal distribution**. It returns an array of the shape $d\\times 1$ (where $d$ = no. of features) containing the initial values of the parameters. In our case $d=4$ (including the ones column).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1620133587060,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "abLGKrPft2JS",
    "outputId": "a1b83393-7e6a-4976-d12c-5d45806de863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.76405235]\n",
      " [0.40015721]\n",
      " [0.97873798]\n",
      " [2.2408932 ]]\n"
     ]
    }
   ],
   "source": [
    "def initialize_betas(X, y):\n",
    "  np.random.seed(0)\n",
    "  betas = np.random.randn(d,1)\n",
    "  return betas\n",
    "\n",
    "betas = initialize_betas(X, y)\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKnXrYVtOxI0"
   },
   "source": [
    "Here, the initial values for our parameters are:\n",
    "\n",
    "$$\\boldsymbol{\\beta} =\\begin{bmatrix}\n",
    "\\beta_0 \\\\ \n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\beta_3\n",
    "\\end{bmatrix} =   \\begin{bmatrix}\n",
    "  1.7640\\\\\n",
    "  0.4001\\\\\n",
    "  0.9787\\\\\n",
    "  2.2408\n",
    " \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOW6cAC9_kBG"
   },
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhcPRsmcvQRG"
   },
   "source": [
    "In OLS, you minimized the sum of squared error (SSE). Here you will be minimizing the cost function. The cost function $J(.)$ is nothing but the sum of squared error multiplied by $\\frac{1}{2}$ to make the derivation easier. You should know that multiplying the cost function with $\\frac{1}{2}$ only changes the value of the cost function but not the optimal parameters that minimize it.\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta_0, \\beta_1, \\beta_2, \\beta_3) &= \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})^2 \\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^{n}((\\beta_0x_{i0}+\\beta_1x_{i1} +\\beta_2x_{i2} + \\beta_3x_{i3})-y_{i})^2\n",
    "\\end{align*}\n",
    "\n",
    "The cost function can be written in matrix form as:\n",
    "\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2}\\ \\sum(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})^2$$\n",
    "\n",
    "\n",
    "*Note: We call $J$ as a function of only parameters $\\boldsymbol{\\beta}$ but not of $X$ and $y$ because $X$ and $y$ are constants given by the dataset. So the value of $J$ depends only on the parameters.*\n",
    "\n",
    "**You want to find the parameters $\\beta_0, \\beta_1, \\beta_2$ and $\\beta_3$ that minimizes the cost function $J$ using Gradient Descent.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liAx8tsai2No"
   },
   "source": [
    "The `calculate_cost` function below calculates the cost function for a particular set of values of parameters `betas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1763,
     "status": "ok",
     "timestamp": 1620133587062,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "s4OEEDOSFdco",
    "outputId": "f30a2ad6-4757-47ac-cfe1-796221a4768a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost with random betas: 2303714.226243876\n"
     ]
    }
   ],
   "source": [
    "def calculate_cost(betas):\n",
    "  cost = 1/2 * np.sum(np.square(np.dot(X, betas)-y))\n",
    "  return cost\n",
    "\n",
    "print(\"Cost with random betas:\", calculate_cost(betas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbvmnpbKj0wk"
   },
   "source": [
    "## Gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G07bkOBORc5Y"
   },
   "source": [
    "You will need to calculate the gradient of the cost function with respect to each of the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxNS4h6TobSc"
   },
   "source": [
    "Partial derivative(gradient) of the cost function with respect to $\\beta_1$, \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\beta_1} &= \\frac{\\partial}{\\partial \\beta_1}\\ \\frac{1}{2}\\ \\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})^2\\\\\n",
    "&=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial}{\\partial \\beta_1}(\\hat{y_{i}}-{y_{i}})^2\n",
    "\\end{align*}\n",
    "\n",
    "$\\hspace{8cm}$Applying chain rule,\n",
    "\n",
    "\\begin{align*}\n",
    "\\hspace{8cm}&=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial (\\hat{y_{i}}-{y_{i}})^2}{\\partial (\\hat{y_{i}}-{y_{i}})} \\times \\frac{\\partial (\\hat{y_{i}}-{y_{i}})}{\\partial \\beta_1}\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i}) \\times \\frac{\\partial (\\beta_0x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3})-y_i)}{\\partial \\beta_1}\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i}) \\times x_{i1}\\\\\n",
    "\\therefore \\frac{\\partial J}{\\partial \\beta_1}&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i1} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MVtD6EAo_1g"
   },
   "source": [
    "Similarly, \n",
    "\n",
    "\\begin{align*}\\frac{\\partial J}{\\partial \\beta_0}&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i0}\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})\\times 1\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_2}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i2}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_3}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i3}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thYWZqkPSQ85"
   },
   "source": [
    "In general, the formula for calculating the gradients with respect to a parameter $\\beta_j$ can be expressed as:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_j}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{ij}$$\n",
    "\n",
    "\n",
    "We can write this generalized expression in matrix form to calculate the gradients wrt. all the parameters simultaneously as:\n",
    "\n",
    "$$\\frac{\\boldsymbol{\\partial J}}{\\boldsymbol{\\partial \\beta}}= \\mathbf{X^T}(\\mathbf{\\hat{y}-y}) = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\beta_0} \\\\ \n",
    "\\frac{\\partial J}{\\partial \\beta_1}\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_2}\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_3}\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Xz0hZVjaMr"
   },
   "source": [
    "The `calculate_gradient` function below calculates the gradients of cost function with respect to the parameters `betas`. It uses the matrix operations to compute the gradient of all the parameters simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1749,
     "status": "ok",
     "timestamp": 1620133587063,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "VGTQFH696hyd",
    "outputId": "86663aab-87c8-4b1f-a297-c672af6466e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for random betas = \n",
      " [[  27563.85598559]\n",
      " [4631129.37253468]\n",
      " [ 731917.2683292 ]\n",
      " [1079270.65268036]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_gradients(betas):\n",
    "  gradients = np.dot(X.T,(np.dot(X,betas)-y))\n",
    "  return gradients\n",
    "\n",
    "print(\"Gradients for random betas = \\n\", calculate_gradients(betas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJigUKetTBqB"
   },
   "source": [
    "Here, the gradients of the cost function with respect to the initial parameters are:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial J}{\\partial \\beta}}  =\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\beta_0} \\\\ \n",
    "\\frac{\\partial J}{\\partial \\beta_1}\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_2}\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_3}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "27563.85 \\\\ \n",
    "4631129.37\\\\\n",
    "731917.26\\\\\n",
    "1079270.65\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_wG80YPmuZG"
   },
   "source": [
    "You can see that the gradients for different parameters vary quite largely. The parameters corresponding to the features having larger values have larger gradients and the parameters corresponding to the features having smaller values have smaller gradients. For example, the gradients with respect to the parameter corresponding to the feature 'TV', $\\frac{\\partial J}{\\partial \\beta_1}$ is larger than the ones corresponing to the 'radio', $\\frac{\\partial J}{\\partial \\beta_2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcyFQ69asVOs"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tVDczBZUNUw"
   },
   "source": [
    "Now you need to update the parameters using their respective gradients until the cost function converges to its minimum value.\n",
    "\n",
    "\n",
    "${\\hspace{5cm}}\\text{Repeat until convergence }\\{$\n",
    "\n",
    "$$\\beta_0 :=\\beta_0-\\alpha\\frac{\\partial J}{\\partial \\beta_0}$$\n",
    "\n",
    "$$\\beta_1 :=\\beta_1-\\alpha\\frac{\\partial J}{\\partial \\beta_1}$$\n",
    "\n",
    "$$\\beta_2 :=\\beta_2-\\alpha\\frac{\\partial J}{\\partial \\beta_2}$$\n",
    "\n",
    "$$\\beta_3 :=\\beta_3-\\alpha\\frac{\\partial J}{\\partial \\beta_3}$$\n",
    "\n",
    "${\\hspace{8cm}}\\}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since you already have a vector $\\beta$ called `beta` containing parameters and an another vector $\\frac{\\partial J}{\\partial \\beta}$ called `gradients` containing the gradients of cost function with respect to the parameters, this updation is a simple matrix operation:\n",
    "\n",
    "$$\\boldsymbol{\\beta} := \\boldsymbol{\\beta} - \\alpha \\boldsymbol{\\frac{\\partial J}{\\partial \\beta}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adoBYUlCk38k"
   },
   "source": [
    "The `gradient_descent` function below applies the gradient descent algorithm to find the optimal parameters `betas` that minimize the cost function. Initially `betas` contain the random initial values of the parameters. It uses the gradients calculated by the `calculate_gradients` function to update the values of the parameters. The default learning rate is set to `alpha=0.003`. The process of updating the parameters is repeated till the cost function decreases by a certain threshold value `precision` or the maximum number of iterations `max_iters` is not reached. Also the list `costs`  contains the values of cost functions for different values of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVha43ihsdqB"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha=0.003 , max_iters=10000, precision = 1e-3):\n",
    "  iteration = 0 # no. of iterations\n",
    "  difference = 1\n",
    "  betas = initialize_betas(X,y) # random initial values of the parameters\n",
    "  cost = calculate_cost(betas) # cost for the initial values pf parameters\n",
    "  costs = [calculate_cost(betas)] # list containing the history of costs for different iterations\n",
    "\n",
    "  while difference > precision and iteration <= max_iters :\n",
    "    # updating the values of parameters\n",
    "    betas = betas - alpha * calculate_gradients(betas)\n",
    "\n",
    "    # cost for the new values of parameters\n",
    "    cost = calculate_cost(betas)\n",
    "\n",
    "    # difference between the cost of current iteration and previous iteration\n",
    "    difference = np.abs(costs[iteration] - cost) \n",
    "    costs.append(cost)\n",
    "    \n",
    "    print(\"iteration: {}, cost: {}\".format(iteration, cost))\n",
    "    iteration += 1\n",
    "    \n",
    "    if(cost == np.infty):\n",
    "      print(\"Cost reached infinity, try smaller learning rate\")\n",
    "      break\n",
    "    \n",
    "  return betas, iteration, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQhroBc1rvzi"
   },
   "source": [
    "Let's use the `gradient_descent` function defined above to learn the parameters for our multiple linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2800,
     "status": "ok",
     "timestamp": 1620133588132,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "M7rZkEHg0jga",
    "outputId": "c5c8da0b-96dc-4187-9b3a-5421df6036b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, cost: 624562462609042.5\n",
      "iteration: 1, cost: 2.0418880086534106e+23\n",
      "iteration: 2, cost: 6.676506400718878e+31\n",
      "iteration: 3, cost: 2.1830649810113797e+40\n",
      "iteration: 4, cost: 7.138123481955472e+48\n",
      "iteration: 5, cost: 2.33400321505963e+57\n",
      "iteration: 6, cost: 7.631657005765802e+65\n",
      "iteration: 7, cost: 2.495377396134661e+74\n",
      "iteration: 8, cost: 8.15931369090003e+82\n",
      "iteration: 9, cost: 2.6679090709738888e+91\n",
      "iteration: 10, cost: 8.723452830258346e+99\n",
      "iteration: 11, cost: 2.852369674426854e+108\n",
      "iteration: 12, cost: 9.326596839463859e+116\n",
      "iteration: 13, cost: 3.0495839787448227e+125\n",
      "iteration: 14, cost: 9.971442535251379e+133\n",
      "iteration: 15, cost: 3.260433781356129e+142\n",
      "iteration: 16, cost: 1.0660873193649942e+151\n",
      "iteration: 17, cost: 3.4858618476162146e+159\n",
      "iteration: 18, cost: 1.1397971441874108e+168\n",
      "iteration: 19, cost: 3.7268761261614086e+176\n",
      "iteration: 20, cost: 1.2186033041567329e+185\n",
      "iteration: 21, cost: 3.984554255714465e+193\n",
      "iteration: 22, cost: 1.3028581625025891e+202\n",
      "iteration: 23, cost: 4.260048383492917e+210\n",
      "iteration: 24, cost: 1.3929384450292801e+219\n",
      "iteration: 25, cost: 4.554590316764685e+227\n",
      "iteration: 26, cost: 1.4892469245567106e+236\n",
      "iteration: 27, cost: 4.869497030584865e+244\n",
      "iteration: 28, cost: 1.5922142218244262e+253\n",
      "iteration: 29, cost: 5.206176556340299e+261\n",
      "iteration: 30, cost: 1.7023007309110772e+270\n",
      "iteration: 31, cost: 5.566134277431088e+278\n",
      "iteration: 32, cost: 1.8199986777783808e+287\n",
      "iteration: 33, cost: 5.950979660238824e+295\n",
      "iteration: 34, cost: 1.9458343211438604e+304\n",
      "iteration: 35, cost: inf\n",
      "Cost reached infinity, try smaller learning rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in square\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "betas, steps, costs = gradient_descent(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pCaStNn22KA"
   },
   "source": [
    "As you can see the default learning rate of `alpha=0.003` seems to be very large for this problem. The reason behind this is that the value of the gradients are very large. When the parameters are updated using these gradients, the parameters too get large values. After some iterations the cost function calculated using these parameters reach infinity.\n",
    "\n",
    "A possible solution to this is scaling the features to small values about which we will learn  in the next chapter. For now, let's further decrease the value of learning rate to `alpha=0.0000003`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2790,
     "status": "ok",
     "timestamp": 1620133588134,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "zSrmNc5s3xPJ",
    "outputId": "eec0b3df-74fa-4fb2-ad6c-46ff1346bc94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, cost: 1605439.1704912463\n",
      "iteration: 1, cost: 1139934.3901561843\n",
      "iteration: 2, cost: 827385.9388970843\n",
      "iteration: 3, cost: 615561.209976966\n",
      "iteration: 4, cost: 470257.51679673046\n",
      "iteration: 5, cost: 369060.2077312669\n",
      "iteration: 6, cost: 297263.3981504857\n",
      "iteration: 7, cost: 245203.88220357336\n",
      "iteration: 8, cost: 206518.5131439004\n",
      "iteration: 9, cost: 177005.05468340195\n",
      "iteration: 10, cost: 153877.402559869\n",
      "iteration: 11, cost: 135278.53927748706\n",
      "iteration: 12, cost: 119961.9354509199\n",
      "iteration: 13, cost: 107083.05179937751\n",
      "iteration: 14, cost: 96062.81371070787\n",
      "iteration: 15, cost: 86498.14156564076\n",
      "iteration: 16, cost: 78103.25292282514\n",
      "iteration: 17, cost: 70671.09390908072\n",
      "iteration: 18, cost: 64047.94353316401\n",
      "iteration: 19, cost: 58116.643622751835\n",
      "iteration: 20, cost: 52785.48135998228\n",
      "iteration: 21, cost: 47980.78022772071\n",
      "iteration: 22, cost: 43641.92759428907\n",
      "iteration: 23, cost: 39718.006670030016\n",
      "iteration: 24, cost: 36165.487872148704\n",
      "iteration: 25, cost: 32946.622472615774\n",
      "iteration: 26, cost: 30028.30423907678\n",
      "iteration: 27, cost: 27381.24512986272\n",
      "iteration: 28, cost: 24979.363686890036\n",
      "iteration: 29, cost: 22799.31920083043\n",
      "iteration: 30, cost: 20820.14728572892\n",
      "iteration: 31, cost: 19022.967302234483\n",
      "iteration: 32, cost: 17390.74179397068\n",
      "iteration: 33, cost: 15908.074504756441\n",
      "iteration: 34, cost: 14561.037772258182\n",
      "iteration: 35, cost: 13337.022896030667\n",
      "iteration: 36, cost: 12224.608945120286\n",
      "iteration: 37, cost: 11213.446723202866\n",
      "iteration: 38, cost: 10294.15545750553\n",
      "iteration: 39, cost: 9458.230358953271\n",
      "iteration: 40, cost: 8697.959605147136\n",
      "iteration: 41, cost: 8006.3495840042415\n",
      "iteration: 42, cost: 7377.057442986674\n",
      "iteration: 43, cost: 6804.330142372723\n",
      "iteration: 44, cost: 6282.949327852188\n",
      "iteration: 45, cost: 5808.181429040497\n",
      "iteration: 46, cost: 5375.732463753651\n",
      "iteration: 47, cost: 4981.7070880655665\n",
      "iteration: 48, cost: 4622.571482663996\n",
      "iteration: 49, cost: 4295.11970914826\n",
      "iteration: 50, cost: 3996.4432072822638\n",
      "iteration: 51, cost: 3723.9031369700047\n",
      "iteration: 52, cost: 3475.105297682545\n",
      "iteration: 53, cost: 3247.877383846185\n",
      "iteration: 54, cost: 3040.248357765963\n",
      "iteration: 55, cost: 2850.4297423695775\n",
      "iteration: 56, cost: 2676.7986547053006\n",
      "iteration: 57, cost: 2517.8824179524054\n",
      "iteration: 58, cost: 2372.3446049044364\n",
      "iteration: 59, cost: 2238.9723796354\n",
      "iteration: 60, cost: 2116.6650165048704\n",
      "iteration: 61, cost: 2004.4234869299344\n",
      "iteration: 62, cost: 1901.3410145647217\n",
      "iteration: 63, cost: 1806.5945087840662\n",
      "iteration: 64, cost: 1719.4367947581557\n",
      "iteration: 65, cost: 1639.1895660118475\n",
      "iteration: 66, cost: 1565.2369922596065\n",
      "iteration: 67, cost: 1497.0199215614045\n",
      "iteration: 68, cost: 1434.0306215166834\n",
      "iteration: 69, cost: 1375.8080093570711\n",
      "iteration: 70, cost: 1321.9333254633075\n",
      "iteration: 71, cost: 1272.0262090624224\n",
      "iteration: 72, cost: 1225.7411386981005\n",
      "iteration: 73, cost: 1182.7642035470376\n",
      "iteration: 74, cost: 1142.8101748101653\n",
      "iteration: 75, cost: 1105.6198492700837\n",
      "iteration: 76, cost: 1070.9576397021706\n",
      "iteration: 77, cost: 1038.6093891814776\n",
      "iteration: 78, cost: 1008.3803884630582\n",
      "iteration: 79, cost: 980.0935775503158\n",
      "iteration: 80, cost: 953.5879143226383\n",
      "iteration: 81, cost: 928.716894686911\n",
      "iteration: 82, cost: 905.3472101625657\n",
      "iteration: 83, cost: 883.357530120471\n",
      "iteration: 84, cost: 862.637397084713\n",
      "iteration: 85, cost: 843.0862245844417\n",
      "iteration: 86, cost: 824.612388020827\n",
      "iteration: 87, cost: 807.1323999010567\n",
      "iteration: 88, cost: 790.5701615956889\n",
      "iteration: 89, cost: 774.8562845052436\n",
      "iteration: 90, cost: 759.9274741836055\n",
      "iteration: 91, cost: 745.7259715659623\n",
      "iteration: 92, cost: 732.199045993307\n",
      "iteration: 93, cost: 719.298535219231\n",
      "iteration: 94, cost: 706.9804280324788\n",
      "iteration: 95, cost: 695.2044855348579\n",
      "iteration: 96, cost: 683.9338974824193\n",
      "iteration: 97, cost: 673.1349704318995\n",
      "iteration: 98, cost: 662.7768447374051\n",
      "iteration: 99, cost: 652.8312377171369\n",
      "iteration: 100, cost: 643.2722105591894\n",
      "iteration: 101, cost: 634.075956761533\n",
      "iteration: 102, cost: 625.2206101063152\n",
      "iteration: 103, cost: 616.686070354582\n",
      "iteration: 104, cost: 608.4538450161845\n",
      "iteration: 105, cost: 600.5069057026183\n",
      "iteration: 106, cost: 592.8295577092808\n",
      "iteration: 107, cost: 585.4073215994827\n",
      "iteration: 108, cost: 578.2268256766812\n",
      "iteration: 109, cost: 571.2757083349287\n",
      "iteration: 110, cost: 564.542529371415\n",
      "iteration: 111, cost: 558.0166894301487\n",
      "iteration: 112, cost: 551.6883568230581\n",
      "iteration: 113, cost: 545.5484010448438\n",
      "iteration: 114, cost: 539.5883323614628\n",
      "iteration: 115, cost: 533.8002469097439\n",
      "iteration: 116, cost: 528.1767767979127\n",
      "iteration: 117, cost: 522.7110447442071\n",
      "iteration: 118, cost: 517.3966228337692\n",
      "iteration: 119, cost: 512.2274950129946\n",
      "iteration: 120, cost: 507.19802297590104\n",
      "iteration: 121, cost: 502.3029151291569\n",
      "iteration: 122, cost: 497.53719835151486\n",
      "iteration: 123, cost: 492.89619228978836\n",
      "iteration: 124, cost: 488.3754859574506\n",
      "iteration: 125, cost: 483.97091642365154\n",
      "iteration: 126, cost: 479.6785494001392\n",
      "iteration: 127, cost: 475.494661551444\n",
      "iteration: 128, cost: 471.4157243698781\n",
      "iteration: 129, cost: 467.4383894716097\n",
      "iteration: 130, cost: 463.5594751833938\n",
      "iteration: 131, cost: 459.77595430163956\n",
      "iteration: 132, cost: 456.0849429164582\n",
      "iteration: 133, cost: 452.4836902032837\n",
      "iteration: 134, cost: 448.9695690936861\n",
      "iteration: 135, cost: 445.54006774517836\n",
      "iteration: 136, cost: 442.19278173724274\n",
      "iteration: 137, cost: 438.9254069275413\n",
      "iteration: 138, cost: 435.7357329083776\n",
      "iteration: 139, cost: 432.621637009025\n",
      "iteration: 140, cost: 429.5810787945586\n",
      "iteration: 141, cost: 426.6120950163901\n",
      "iteration: 142, cost: 423.7127949738404\n",
      "iteration: 143, cost: 420.881356249838\n",
      "iteration: 144, cost: 418.11602078723377\n",
      "iteration: 145, cost: 415.4150912753131\n",
      "iteration: 146, cost: 412.7769278188828\n",
      "iteration: 147, cost: 410.1999448648585\n",
      "iteration: 148, cost: 407.68260836357814\n",
      "iteration: 149, cost: 405.22343314416196\n",
      "iteration: 150, cost: 402.820980485134\n",
      "iteration: 151, cost: 400.47385586324833\n",
      "iteration: 152, cost: 398.1807068650165\n",
      "iteration: 153, cost: 395.9402212468582\n",
      "iteration: 154, cost: 393.7511251310785\n",
      "iteration: 155, cost: 391.61218132604415\n",
      "iteration: 156, cost: 389.52218775998824\n",
      "iteration: 157, cost: 387.4799760188376\n",
      "iteration: 158, cost: 385.4844099793246\n",
      "iteration: 159, cost: 383.53438452943954\n",
      "iteration: 160, cost: 381.6288243689976\n",
      "iteration: 161, cost: 379.76668288374526\n",
      "iteration: 162, cost: 377.9469410870238\n",
      "iteration: 163, cost: 376.16860662354554\n",
      "iteration: 164, cost: 374.43071283032293\n",
      "iteration: 165, cost: 372.7323178502367\n",
      "iteration: 166, cost: 371.0725037941303\n",
      "iteration: 167, cost: 369.4503759476812\n",
      "iteration: 168, cost: 367.8650620196321\n",
      "iteration: 169, cost: 366.3157114282685\n",
      "iteration: 170, cost: 364.80149462329655\n",
      "iteration: 171, cost: 363.3216024405298\n",
      "iteration: 172, cost: 361.87524548701487\n",
      "iteration: 173, cost: 360.4616535544326\n",
      "iteration: 174, cost: 359.08007505879675\n",
      "iteration: 175, cost: 357.72977650464225\n",
      "iteration: 176, cost: 356.4100419720488\n",
      "iteration: 177, cost: 355.1201726249845\n",
      "iteration: 178, cost: 353.8594862395823\n",
      "iteration: 179, cost: 352.62731675107733\n",
      "iteration: 180, cost: 351.42301381823904\n",
      "iteration: 181, cost: 350.2459424042251\n",
      "iteration: 182, cost: 349.0954823728766\n",
      "iteration: 183, cost: 347.97102809954526\n",
      "iteration: 184, cost: 346.8719880956251\n",
      "iteration: 185, cost: 345.7977846460176\n",
      "iteration: 186, cost: 344.7478534588257\n",
      "iteration: 187, cost: 343.7216433266238\n",
      "iteration: 188, cost: 342.71861579870074\n",
      "iteration: 189, cost: 341.73824486371905\n",
      "iteration: 190, cost: 340.7800166422734\n",
      "iteration: 191, cost: 339.8434290888722\n",
      "iteration: 192, cost: 338.9279917028965\n",
      "iteration: 193, cost: 338.03322524812586\n",
      "iteration: 194, cost: 337.1586614804457\n",
      "iteration: 195, cost: 336.3038428833819\n",
      "iteration: 196, cost: 335.4683224111273\n",
      "iteration: 197, cost: 334.6516632387499\n",
      "iteration: 198, cost: 333.85343851929326\n",
      "iteration: 199, cost: 333.0732311474955\n",
      "iteration: 200, cost: 332.31063352987167\n",
      "iteration: 201, cost: 331.56524736092155\n",
      "iteration: 202, cost: 330.83668340523604\n",
      "iteration: 203, cost: 330.12456128529055\n",
      "iteration: 204, cost: 329.4285092747255\n",
      "iteration: 205, cost: 328.74816409692585\n",
      "iteration: 206, cost: 328.08317072872\n",
      "iteration: 207, cost: 327.43318220902955\n",
      "iteration: 208, cost: 326.7978594523106\n",
      "iteration: 209, cost: 326.17687106663305\n",
      "iteration: 210, cost: 325.5698931762547\n",
      "iteration: 211, cost: 324.97660924855177\n",
      "iteration: 212, cost: 324.39670992517466\n",
      "iteration: 213, cost: 323.82989285730525\n",
      "iteration: 214, cost: 323.27586254489347\n",
      "iteration: 215, cost: 322.73433017976265\n",
      "iteration: 216, cost: 322.20501349247036\n",
      "iteration: 217, cost: 321.68763660282207\n",
      "iteration: 218, cost: 321.1819298739372\n",
      "iteration: 219, cost: 320.68762976976905\n",
      "iteration: 220, cost: 320.2044787159871\n",
      "iteration: 221, cost: 319.7322249641322\n",
      "iteration: 222, cost: 319.2706224589574\n",
      "iteration: 223, cost: 318.8194307088726\n",
      "iteration: 224, cost: 318.37841465941096\n",
      "iteration: 225, cost: 317.9473445696428\n",
      "iteration: 226, cost: 317.52599589145814\n",
      "iteration: 227, cost: 317.11414915164937\n",
      "iteration: 228, cost: 316.7115898367206\n",
      "iteration: 229, cost: 316.31810828035884\n",
      "iteration: 230, cost: 315.93349955349936\n",
      "iteration: 231, cost: 315.5575633569224\n",
      "iteration: 232, cost: 315.19010391631866\n",
      "iteration: 233, cost: 314.8309298797652\n",
      "iteration: 234, cost: 314.47985421755186\n",
      "iteration: 235, cost: 314.13669412430175\n",
      "iteration: 236, cost: 313.80127092333345\n",
      "iteration: 237, cost: 313.4734099732069\n",
      "iteration: 238, cost: 313.15294057640614\n",
      "iteration: 239, cost: 312.839695890104\n",
      "iteration: 240, cost: 312.5335128389628\n",
      "iteration: 241, cost: 312.23423202992115\n",
      "iteration: 242, cost: 311.9416976689215\n",
      "iteration: 243, cost: 311.65575747953244\n",
      "iteration: 244, cost: 311.37626262342104\n",
      "iteration: 245, cost: 311.10306762263406\n",
      "iteration: 246, cost: 310.8360302836436\n",
      "iteration: 247, cost: 310.5750116231183\n",
      "iteration: 248, cost: 310.31987579537855\n",
      "iteration: 249, cost: 310.0704900214985\n",
      "iteration: 250, cost: 309.82672452001515\n",
      "iteration: 251, cost: 309.588452439209\n",
      "iteration: 252, cost: 309.35554979091893\n",
      "iteration: 253, cost: 309.1278953858574\n",
      "iteration: 254, cost: 308.90537077039005\n",
      "iteration: 255, cost: 308.6878601647477\n",
      "iteration: 256, cost: 308.4752504026363\n",
      "iteration: 257, cost: 308.2674308722146\n",
      "iteration: 258, cost: 308.0642934584073\n",
      "iteration: 259, cost: 307.8657324865229\n",
      "iteration: 260, cost: 307.67164466714786\n",
      "iteration: 261, cost: 307.4819290422862\n",
      "iteration: 262, cost: 307.2964869327176\n",
      "iteration: 263, cost: 307.1152218865461\n",
      "iteration: 264, cost: 306.9380396289114\n",
      "iteration: 265, cost: 306.76484801283704\n",
      "iteration: 266, cost: 306.5955569711899\n",
      "iteration: 267, cost: 306.43007846972534\n",
      "iteration: 268, cost: 306.26832646119306\n",
      "iteration: 269, cost: 306.11021684048\n",
      "iteration: 270, cost: 305.95566740076765\n",
      "iteration: 271, cost: 305.80459779067877\n",
      "iteration: 272, cost: 305.65692947239216\n",
      "iteration: 273, cost: 305.51258568070494\n",
      "iteration: 274, cost: 305.37149138301777\n",
      "iteration: 275, cost: 305.2335732402249\n",
      "iteration: 276, cost: 305.0987595684878\n",
      "iteration: 277, cost: 304.96698030187065\n",
      "iteration: 278, cost: 304.83816695582135\n",
      "iteration: 279, cost: 304.71225259147565\n",
      "iteration: 280, cost: 304.589171780769\n",
      "iteration: 281, cost: 304.4688605723344\n",
      "iteration: 282, cost: 304.35125645817163\n",
      "iteration: 283, cost: 304.2362983410682\n",
      "iteration: 284, cost: 304.1239265027564\n",
      "iteration: 285, cost: 304.0140825727889\n",
      "iteration: 286, cost: 303.9067094981173\n",
      "iteration: 287, cost: 303.80175151335743\n",
      "iteration: 288, cost: 303.69915411172565\n",
      "iteration: 289, cost: 303.5988640166318\n",
      "iteration: 290, cost: 303.5008291539129\n",
      "iteration: 291, cost: 303.4049986246939\n",
      "iteration: 292, cost: 303.31132267886125\n",
      "iteration: 293, cost: 303.2197526891348\n",
      "iteration: 294, cost: 303.13024112572566\n",
      "iteration: 295, cost: 303.0427415315651\n",
      "iteration: 296, cost: 302.9572084980939\n",
      "iteration: 297, cost: 302.87359764159703\n",
      "iteration: 298, cost: 302.7918655800733\n",
      "iteration: 299, cost: 302.71196991062635\n",
      "iteration: 300, cost: 302.63386918736694\n",
      "iteration: 301, cost: 302.5575228998134\n",
      "iteration: 302, cost: 302.4828914517793\n",
      "iteration: 303, cost: 302.4099361407385\n",
      "iteration: 304, cost: 302.33861913765537\n",
      "iteration: 305, cost: 302.2689034672704\n",
      "iteration: 306, cost: 302.2007529888308\n",
      "iteration: 307, cost: 302.13413237725615\n",
      "iteration: 308, cost: 302.0690071047288\n",
      "iteration: 309, cost: 302.0053434227003\n",
      "iteration: 310, cost: 301.9431083443034\n",
      "iteration: 311, cost: 301.88226962716135\n",
      "iteration: 312, cost: 301.8227957565852\n",
      "iteration: 313, cost: 301.76465592915025\n",
      "iteration: 314, cost: 301.70782003664334\n",
      "iteration: 315, cost: 301.6522586503719\n",
      "iteration: 316, cost: 301.5979430058282\n",
      "iteration: 317, cost: 301.5448449876983\n",
      "iteration: 318, cost: 301.49293711521113\n",
      "iteration: 319, cost: 301.44219252781613\n",
      "iteration: 320, cost: 301.3925849711862\n",
      "iteration: 321, cost: 301.3440887835351\n",
      "iteration: 322, cost: 301.29667888224435\n",
      "iteration: 323, cost: 301.2503307507918\n",
      "iteration: 324, cost: 301.20502042597525\n",
      "iteration: 325, cost: 301.160724485424\n",
      "iteration: 326, cost: 301.1174200353934\n",
      "iteration: 327, cost: 301.0750846988334\n",
      "iteration: 328, cost: 301.03369660372744\n",
      "iteration: 329, cost: 300.99323437169426\n",
      "iteration: 330, cost: 300.9536771068465\n",
      "iteration: 331, cost: 300.91500438490186\n",
      "iteration: 332, cost: 300.8771962425384\n",
      "iteration: 333, cost: 300.8402331669917\n",
      "iteration: 334, cost: 300.80409608588593\n",
      "iteration: 335, cost: 300.76876635729445\n",
      "iteration: 336, cost: 300.73422576002486\n",
      "iteration: 337, cost: 300.7004564841236\n",
      "iteration: 338, cost: 300.66744112159415\n",
      "iteration: 339, cost: 300.63516265732574\n",
      "iteration: 340, cost: 300.6036044602256\n",
      "iteration: 341, cost: 300.57275027455245\n",
      "iteration: 342, cost: 300.5425842114447\n",
      "iteration: 343, cost: 300.51309074064045\n",
      "iteration: 344, cost: 300.4842546823843\n",
      "iteration: 345, cost: 300.4560611995162\n",
      "iteration: 346, cost: 300.42849578974\n",
      "iteration: 347, cost: 300.40154427806505\n",
      "iteration: 348, cost: 300.3751928094199\n",
      "iteration: 349, cost: 300.34942784143124\n",
      "iteration: 350, cost: 300.3242361373673\n",
      "iteration: 351, cost: 300.29960475923923\n",
      "iteration: 352, cost: 300.27552106105867\n",
      "iteration: 353, cost: 300.2519726822479\n",
      "iteration: 354, cost: 300.22894754119795\n",
      "iteration: 355, cost: 300.20643382897254\n",
      "iteration: 356, cost: 300.1844200031546\n",
      "iteration: 357, cost: 300.1628947818301\n",
      "iteration: 358, cost: 300.14184713770993\n",
      "iteration: 359, cost: 300.1212662923819\n",
      "iteration: 360, cost: 300.1011417106949\n",
      "iteration: 361, cost: 300.0814630952681\n",
      "iteration: 362, cost: 300.0622203811246\n",
      "iteration: 363, cost: 300.0434037304466\n",
      "iteration: 364, cost: 300.02500352744835\n",
      "iteration: 365, cost: 300.0070103733653\n",
      "iteration: 366, cost: 299.9894150815554\n",
      "iteration: 367, cost: 299.9722086727128\n",
      "iteration: 368, cost: 299.9553823701873\n",
      "iteration: 369, cost: 299.9389275954113\n",
      "iteration: 370, cost: 299.92283596342884\n",
      "iteration: 371, cost: 299.90709927852623\n",
      "iteration: 372, cost: 299.8917095299606\n",
      "iteration: 373, cost: 299.8766588877859\n",
      "iteration: 374, cost: 299.8619396987714\n",
      "iteration: 375, cost: 299.8475444824146\n",
      "iteration: 376, cost: 299.8334659270419\n",
      "iteration: 377, cost: 299.81969688599895\n",
      "iteration: 378, cost: 299.8062303739256\n",
      "iteration: 379, cost: 299.79305956311634\n",
      "iteration: 380, cost: 299.7801777799615\n",
      "iteration: 381, cost: 299.76757850147\n",
      "iteration: 382, cost: 299.75525535186955\n",
      "iteration: 383, cost: 299.7432020992843\n",
      "iteration: 384, cost: 299.73141265248705\n",
      "iteration: 385, cost: 299.719881057725\n",
      "iteration: 386, cost: 299.70860149561713\n",
      "iteration: 387, cost: 299.6975682781214\n",
      "iteration: 388, cost: 299.6867758455705\n",
      "iteration: 389, cost: 299.67621876377456\n",
      "iteration: 390, cost: 299.6658917211895\n",
      "iteration: 391, cost: 299.6557895261485\n",
      "iteration: 392, cost: 299.6459071041568\n",
      "iteration: 393, cost: 299.6362394952473\n",
      "iteration: 394, cost: 299.62678185139544\n",
      "iteration: 395, cost: 299.6175294339929\n",
      "iteration: 396, cost: 299.6084776113783\n",
      "iteration: 397, cost: 299.59962185642314\n",
      "iteration: 398, cost: 299.5909577441729\n",
      "iteration: 399, cost: 299.58248094954087\n",
      "iteration: 400, cost: 299.5741872450544\n",
      "iteration: 401, cost: 299.56607249865175\n",
      "iteration: 402, cost: 299.55813267152837\n",
      "iteration: 403, cost: 299.5503638160326\n",
      "iteration: 404, cost: 299.5427620736085\n",
      "iteration: 405, cost: 299.53532367278456\n",
      "iteration: 406, cost: 299.5280449272085\n",
      "iteration: 407, cost: 299.5209222337266\n",
      "iteration: 408, cost: 299.51395207050507\n",
      "iteration: 409, cost: 299.50713099519555\n",
      "iteration: 410, cost: 299.50045564314087\n",
      "iteration: 411, cost: 299.4939227256215\n",
      "iteration: 412, cost: 299.4875290281418\n",
      "iteration: 413, cost: 299.4812714087551\n",
      "iteration: 414, cost: 299.475146796426\n",
      "iteration: 415, cost: 299.4691521894296\n",
      "iteration: 416, cost: 299.4632846537884\n",
      "iteration: 417, cost: 299.45754132174187\n",
      "iteration: 418, cost: 299.4519193902532\n",
      "iteration: 419, cost: 299.44641611954756\n",
      "iteration: 420, cost: 299.4410288316851\n",
      "iteration: 421, cost: 299.43575490916487\n",
      "iteration: 422, cost: 299.4305917935609\n",
      "iteration: 423, cost: 299.42553698418914\n",
      "iteration: 424, cost: 299.42058803680385\n",
      "iteration: 425, cost: 299.4157425623241\n",
      "iteration: 426, cost: 299.41099822558886\n",
      "iteration: 427, cost: 299.4063527441399\n",
      "iteration: 428, cost: 299.40180388703226\n",
      "iteration: 429, cost: 299.3973494736717\n",
      "iteration: 430, cost: 299.39298737267825\n",
      "iteration: 431, cost: 299.38871550077573\n",
      "iteration: 432, cost: 299.384531821706\n",
      "iteration: 433, cost: 299.38043434516754\n",
      "iteration: 434, cost: 299.37642112577845\n",
      "iteration: 435, cost: 299.3724902620627\n",
      "iteration: 436, cost: 299.36863989545907\n",
      "iteration: 437, cost: 299.3648682093527\n",
      "iteration: 438, cost: 299.3611734281283\n",
      "iteration: 439, cost: 299.35755381624494\n",
      "iteration: 440, cost: 299.3540076773314\n",
      "iteration: 441, cost: 299.3505333533026\n",
      "iteration: 442, cost: 299.34712922349456\n",
      "iteration: 443, cost: 299.3437937038212\n",
      "iteration: 444, cost: 299.34052524594756\n",
      "iteration: 445, cost: 299.33732233648345\n",
      "iteration: 446, cost: 299.334183496195\n",
      "iteration: 447, cost: 299.3311072792334\n",
      "iteration: 448, cost: 299.32809227238147\n",
      "iteration: 449, cost: 299.3251370943174\n",
      "iteration: 450, cost: 299.32224039489466\n",
      "iteration: 451, cost: 299.31940085443864\n",
      "iteration: 452, cost: 299.3166171830586\n",
      "iteration: 453, cost: 299.31388811997573\n",
      "iteration: 454, cost: 299.31121243286617\n",
      "iteration: 455, cost: 299.30858891721846\n",
      "iteration: 456, cost: 299.30601639570614\n",
      "iteration: 457, cost: 299.3034937175742\n",
      "iteration: 458, cost: 299.3010197580392\n",
      "iteration: 459, cost: 299.29859341770316\n",
      "iteration: 460, cost: 299.2962136219809\n",
      "iteration: 461, cost: 299.2938793205394\n",
      "iteration: 462, cost: 299.29158948675143\n",
      "iteration: 463, cost: 299.2893431171594\n",
      "iteration: 464, cost: 299.28713923095324\n",
      "iteration: 465, cost: 299.28497686945923\n",
      "iteration: 466, cost: 299.28285509563983\n",
      "iteration: 467, cost: 299.2807729936059\n",
      "iteration: 468, cost: 299.27872966813914\n",
      "iteration: 469, cost: 299.2767242442257\n",
      "iteration: 470, cost: 299.27475586660023\n",
      "iteration: 471, cost: 299.27282369929986\n",
      "iteration: 472, cost: 299.27092692522893\n",
      "iteration: 473, cost: 299.269064745733\n",
      "iteration: 474, cost: 299.26723638018234\n",
      "iteration: 475, cost: 299.26544106556594\n",
      "iteration: 476, cost: 299.26367805609317\n",
      "iteration: 477, cost: 299.26194662280517\n",
      "iteration: 478, cost: 299.26024605319515\n",
      "iteration: 479, cost: 299.2585756508373\n",
      "iteration: 480, cost: 299.2569347350235\n",
      "iteration: 481, cost: 299.2553226404086\n",
      "iteration: 482, cost: 299.2537387166638\n",
      "iteration: 483, cost: 299.25218232813774\n",
      "iteration: 484, cost: 299.25065285352537\n",
      "iteration: 485, cost: 299.24914968554384\n",
      "iteration: 486, cost: 299.2476722306162\n",
      "iteration: 487, cost: 299.24621990856224\n",
      "iteration: 488, cost: 299.2447921522959\n",
      "iteration: 489, cost: 299.2433884075298\n",
      "iteration: 490, cost: 299.2420081324863\n",
      "iteration: 491, cost: 299.2406507976157\n",
      "iteration: 492, cost: 299.2393158853191\n",
      "iteration: 493, cost: 299.23800288968016\n",
      "iteration: 494, cost: 299.2367113162001\n",
      "iteration: 495, cost: 299.2354406815411\n",
      "iteration: 496, cost: 299.2341905132736\n",
      "iteration: 497, cost: 299.2329603496303\n",
      "iteration: 498, cost: 299.231749739266\n",
      "iteration: 499, cost: 299.23055824102175\n",
      "iteration: 500, cost: 299.22938542369536\n",
      "iteration: 501, cost: 299.2282308658165\n",
      "iteration: 502, cost: 299.2270941554269\n",
      "iteration: 503, cost: 299.2259748898663\n",
      "iteration: 504, cost: 299.2248726755618\n",
      "iteration: 505, cost: 299.22378712782336\n",
      "iteration: 506, cost: 299.22271787064307\n",
      "iteration: 507, cost: 299.22166453649896\n",
      "iteration: 508, cost: 299.2206267661642\n",
      "iteration: 509, cost: 299.219604208519\n",
      "iteration: 510, cost: 299.2185965203684\n",
      "iteration: 511, cost: 299.217603366263\n"
     ]
    }
   ],
   "source": [
    "betas, steps, costs = gradient_descent(X, y, alpha=0.0000003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Cnq33Adv1RD"
   },
   "source": [
    "If we plot the cost function $J$ against the number of iteration, we get a plot as shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 2780,
     "status": "ok",
     "timestamp": 1620133588136,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "dd0cgF4ADmhG",
    "outputId": "433904c2-7f3b-4e8a-e675-d1506fe8f5f3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)\n",
    "plt.xlabel(\"No. of iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.ylim(0, 2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUZ_WKyJyqev"
   },
   "source": [
    "As you can see, the cost function $J$ decreases with number of iterations and finally saturates around $299$ in $511^{th}$ iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53s4STK-wU3o"
   },
   "source": [
    "Let's see the optimal parameters found by Gradient Descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "executionInfo": {
     "elapsed": 2768,
     "status": "ok",
     "timestamp": 1620133588137,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "1vOc-SqbwhqW",
    "outputId": "87b4f55c-91b8-4ce6-a1bf-e4b83ca11feb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>TV, Radio, and Newspaper</strong> <br>\n",
       "$y$ = 1.75 + 0.05$x_{1}$ + 0.20$x_{2}$ + 0.01$x_{3}$ <br>\n",
       "$x_{1}$ = TV <br>\n",
       "$x_{2}$ = radio <br>\n",
       "$x_{3}$ = newspaper\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "message = \"\"\"<strong>TV, Radio, and Newspaper</strong> <br>\n",
    "$y$ = {:.2f} + {:.2f}$x_{{1}}$ + {:.2f}$x_{{2}}$ + {:.2f}$x_{{3}}$ <br>\n",
    "$x_{{1}}$ = TV <br>\n",
    "$x_{{2}}$ = radio <br>\n",
    "$x_{{3}}$ = newspaper\n",
    "\"\"\".format(*betas[0], *betas[1], *betas[2], *betas[3])\n",
    "display(HTML( message ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOvWRz0tGrPN"
   },
   "source": [
    "*TV, radio* and *newspaper* are the input variables, $x_1, x_2$ and $x_3$ respectively and _sales_ is the output variable, $y$. We obtain a multiple linear regression model of $y = 1.75 + 0.05x_{1} + 0.20x_{2} + 0.01x_{3}$. Note that each of the estimated model parameters (i.e., $\\beta_{0}$ through $\\beta_{3}$) have been rounded to 2 decimal places.. Intercept, $\\beta_0$ has been estimated as $1.75$ and three regression coefficients, $\\beta_1$, $\\beta_2$ and $\\beta_3$ have been estimated to $0.05$, $0.20$,  and $0.01$ respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T4RprYQ20uf"
   },
   "source": [
    "You might have noticed that the parameters found by Gradient Descent are similar but not exactly the same as that found by OLS. This is because the Gradient Descent depends on the factors such as the learning rate and the number of iterations for which it runs. Selection of these factors heavily affects the solution that the Gradient Descent finds. \n",
    "\n",
    "*You can play around with the learning rate and the number of iterations and see if the soilution matches to that of OLS* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmiCEuhOobGU"
   },
   "source": [
    "## Comparison with Ordinary Least Squares method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA81KOHEuDeX"
   },
   "source": [
    "While both of the techniques find the parameters that minimize the cost function, their approach is quite different. As mentioned earlier, Ordinary Least Squares (OLS) is a direct method to find the optimal parameters whereas the Gradient Descent is an iterative method. This is the main difference between the two approaches. Few other points that differentiate the two approaches are:\n",
    "\n",
    "* You don't need to choose any hyperparameter in OLS whereas Gradient Descent requires you to choose a learning rate and the number of iterations.\n",
    "\n",
    "* OLS requires the number of samples $n$ to be greater than the number of features $d$. Also it requires $(\\mathbf{X}^{T}\\mathbf{X})^{-1}$ to exist. However, there are no such constraints in Gradient Descent.\n",
    "\n",
    "* If the constraints of OLS are met then it always gives the exact solution. Whereas the solution provided by the Gradient descent is dependent on the learning rate and the number of iterations and may not always be exact.\n",
    "\n",
    "* The time complexity of OLS is $O(d^3)$ whereas the time complexity of Gradient Descent is $O(kd^2)$ where $k$ is the number of iterations. So if the number of features is large (exceeding $10,000$), then its a good idea to choose Gradient Descent.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJXZe1kBoJCn"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "* Gradient of the cost function w.r.t each of the parameters can be derived easily using calculus.\n",
    "\n",
    "* The parameters are updated iteratively using their corresponding gradients.\n",
    "\n",
    "* When the dataset is large with a large number of features, Gradient descent is preferred instead of OLS.\n",
    "of the function.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Read: Gradient Descent for Linear Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
