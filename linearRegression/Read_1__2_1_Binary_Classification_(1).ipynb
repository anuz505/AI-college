{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "woZD3Sv36wVM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites:\n",
        "- Logistic regression basics\n",
        "- Probability\n",
        "- Logarithm and exponents\n",
        "- Linear algebra\n",
        "- Calculus"
      ],
      "metadata": {
        "id": "woZD3Sv36wVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Objectives:\n",
        "- Revisit Sigmoid function\n",
        "- Establish Binary cross entropy as cost function\n",
        "- Optimize cost function with Gradient Descent\n"
      ],
      "metadata": {
        "id": "jSobfHNB8TFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous unit, we learned about supervised learning methods to predict continuous data using linear regression as well as discrete data using logistic regression. In this notebook, we will expound on the sigmoid function for binary classification and cost function for logistic regression as well as optimize the parameters using gradient descent.\n",
        "\n",
        "The model for the Logistic regression is similar to the model of the linear regression as both models compute a weighted sum of the input features. However, in logistic regression, the output of the linear model is passed through a logistic function (sigmoid function) to give the _logistic_ of the outputs.\n",
        "\n",
        "$$ \\mathcal{\\hat{P}} = h_w(x) = \\sigma(w^Tx) $$"
      ],
      "metadata": {
        "id": "i3LXANMQk8Yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Revisiting the Sigmoid Function\n",
        "\n",
        "The sigmoid function is an S-shaped curve that is used to classify a given sample into positive or negative classes and is defined by the expression :\n",
        "\n",
        "$$ \\sigma(w^Tx) = \\frac{1}{1 + e^{-w^Tx}} $$\n",
        "\n",
        "<div align='center'>\n",
        "\n",
        "![logistic](https://drive.google.com/uc?id=16iX4PEywb5Od9IMOXfMFL1q20HMDInfQ)\n",
        "\n",
        "<figcaption> Figure 1: Sigmoid Activation Function</figcaption>\n",
        "\n",
        "</div>\n",
        "\n",
        "The sigmoid function, also known as the logistic function, $\\sigma(x)$ maps every positive value of $x$ towards $1$, positive class, and every negative value of $x$ towards $0$, negative class. As the output of the function is bound within $(0,1)$, it is a perfect model for probability.\n",
        "\n",
        "We can easily predict of the class after selecting an appropriate threshold. Let's take $0.5$ to be the threshold, resulting in the prediction $\\hat{y}$:\n",
        "\n",
        "$$ \\hat{y} =\n",
        "\\begin{cases}\n",
        "  1 & \\text{if} & \\mathcal{\\hat{P}} \\ge 0.5 \\\\\n",
        "  0 & \\text{if} & \\mathcal{\\hat{P}} < 0.5\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Since, $\\sigma(x) < 0.5$ for all $x< 0$ and $\\sigma(x) \\ge 0.5 $ for all $x \\ge 0$, the logistic regression model will predict $1$ if $w^Tx$ is positive and $0$ if it is negative."
      ],
      "metadata": {
        "id": "rmq7ZoYO6uks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cost function\n",
        "In linear regression, we expressed the cost function as\n",
        "\\begin{equation*}\n",
        "\\mathcal{J}(w) = \\frac{1}{m} \\sum_{i=1}^m \\frac{1}{2} (h_w(x^{(i)}) - y^{(i)})^2\n",
        "\\end{equation*}\n",
        "\n",
        "The sum of squared errors for logistic regression is a non-convex function thus the gradient descent won't converge to a global minimum. Therefore, SSE is not fit as a cost function for the logistic regression model.\n",
        "\n",
        "|![convex](https://drive.google.com/uc?id=1lPxYbWqr7wXqJxG2ToBNZxWii_9gea__) | ![non-convex](https://drive.google.com/uc?id=1Ut38s1JYMS_jbe29VQctOYTOq-YSaS62) |\n",
        "|-|-|\n",
        "\n",
        "<center><figcaption>Figure 2: Convex and Non-convex function</figcaption></center>\n",
        "\n",
        "The logistic regression model, $ h_w(x) $, can be used to make a prediction as it estimates the probabilities.\n",
        "\n",
        "$$ \\mathcal{\\hat{P}} = h_w(x) = \\sigma(w^Tx) $$\n",
        "\n",
        " We now need to choose the parameters $w$ such that, the model $h_w(x)$ estimates high probabilities for positive class $(y=1)$ and low probabilities for negative class $(y=0)$. The following cost function captures this behavior of $h_w(x)$.\n",
        "\n",
        "<a name='eq1'></a>\n",
        "$$\n",
        "\\text{Cost}(h_w(x),y) =\n",
        "\\begin{cases}\n",
        "  -\\log (h_w(x)) & \\text{if} & y = 1 \\\\\n",
        "  -\\log (1-h_w(x)) & \\text{if} & y = 0\n",
        "\\end{cases}   \\tag{Equation 1}\n",
        "$$\n",
        "\n",
        "\n",
        "For positive class, this cost function will give a large value if the model estimates a probability close to $0$, as $-\\log(h_w(x)$ increases when $h_w(x)$ approaches  $0$, similarly the cost will be very large if the model estimates a probability close to $1$ for the negative class. Likewise, when the model estimates a probability close to $1$ for positive class, the cost is close to $0$, and the cost is near $0$ for negative class when the model gives a probability close to $0$.\n",
        "\n",
        "The following plots illustrate the behavior of the cost function with respect to the prediction of the model for each class.\n",
        "\n",
        "\n",
        "\n",
        "<div align='center'>\n",
        "\n",
        "![cost function](https://drive.google.com/uc?id=1P9LYnXXkH-mYqXgz9FKGweR_HB55vv8R)\n",
        "\n",
        "<figcaption>Figure 3: Behavour of Cost function for (Positive Class) $y=1$ and (Negative class) $y=0$ </figcaption>\n",
        "</div>\n",
        "\n",
        "If $y=1$, the cost function is given as\n",
        "$$\\text{Cost}(h_w(x),y) = -\\log (h_w(x))$$\n",
        "\n",
        "From above diagram, we can observe that the $\\text{Cost}=0$, at $h_w(x) = 1$, which is correctly classified as the positive class. For the wrong prediction, the cost becomes extremely large as shown by the red dotted line at $h_w(x)=0$.\n",
        "\n",
        "Thus $h_w(x)=0$ is similar to predicting $\\mathcal{P}(y=1|x;w)=0$:  the probability of positive class prediction $(y=1)$ given $x$ with $w$ parameters is zero.\n",
        "\n",
        "Similarly, if $y=0$, the cost function is given as\n",
        "$$\\text{Cost}(h_w(x),y) = -\\log (1- h_w(x))$$\n",
        "\n",
        "and above plot shows $\\text{Cost}=0$, at $h_w(x) = 0$, which is the correct prediction for the negative class. When the model predicts a positive class at $(1-h_w(x)) = 1$, the cost is extremely high.\n"
      ],
      "metadata": {
        "id": "wYFZeYezrdDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, the cost function can be written in a single line as:\n",
        "<a name=\"eq2\"></a>\n",
        "$$ \\text{Cost}(h_w(x), y) = -y\\log(h_w(x)) - (1-y)\\log(1-h_w(x)) \\tag{Equation 2}$$\n",
        "\n",
        "[This equation](#eq2) is similar to the [Equation 1](#eq1) and behaves in similar manner.\n",
        "- If $y=1$, $(1-y)$ term will become zero, therefore $-\\log(h_w(x))$ alone will be present.\n",
        "- Similarly, if $y=0$, $y$ term will become zero, therefore only $-\\log(1-h_w(x))$ will be present in the equation."
      ],
      "metadata": {
        "id": "aUAPvh8kcyZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalized Binary Cross-entropy\n",
        "The cost function across all examples ( let's say $m$ in size) thus can be represented with a single _log loss_ expression as:\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathcal{J}(w) &= - \\frac{1}{m} \\sum_{i=1}^m y_i \\log(h_w(x_i)) + (1-y_i) \\log(1-h_w(x_i)) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "Since, $\\mathcal{\\hat{P}} = h_w(x_i) = \\sigma(w^Tx_i)$, the cost function can also be expressed as\n",
        "\\begin{align*}\n",
        " \\mathcal{J}(w) &=- \\frac{1}{m} \\sum_{i=1}^m y_i \\log (\\sigma(w^T x_i)) + (1-y_i) \\log (1-\\sigma(w^Tx_i) ) \\\\\n",
        " &=- \\frac{1}{m} \\sum_{i=1}^m y_i \\log (\\mathcal{\\hat{P}}(1|x_i;w))+ (1-y_i) \\log (1-\\mathcal{\\hat{P}}(1|x_i;w) ) \\\\\n",
        " &=- \\frac{1}{m} \\sum_{i=1}^m y_i \\log (\\mathcal{\\hat{P}}(1|x_i;w))+ (1-y_i) \\log (\\mathcal{\\hat{P}}(0|x_i;w) ) \\tag{Equation 3}\n",
        "\\end{align*}\n",
        "\n",
        "Equation 3 is known as the **Normalized Binary Cross Entropy (BCE)**, which can be derived using the likelihood function. It is discussed in detail later in the Probabilistic Methods module.\n",
        "\n",
        "Equation 3 has no known closed-form solution to find the parameters $w$ that minimizes the cost function (as Least square for linear regression). However this function is a convex function ([the proof of convexity of this function](https://towardsdatascience.com/binary-cross-entropy-and-logistic-regression-bf7098e75559)), so any optimization algorithm such as Gradient Descent will find the global minimum provided the right set of hyperparameters (learning rate and number of iterations)."
      ],
      "metadata": {
        "id": "93c5n-h870yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing parameters: Gradient Descent on Binary cross entropy"
      ],
      "metadata": {
        "id": "jQ3_gYxEBDyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let $z = w_1 x_1 + w_2 x_2 + b$ be our linear model, then let $ \\hat{y} = a = \\sigma(z) $ be our logistic model.\n",
        "\n",
        "Also, let $\\mathcal{J}(\\hat{y}, y)$ represent our log-loss function where $\\hat{y}$ is the predicted class and $y$ is the actual class.\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathcal{J}(\\hat{y}, y) = - y \\log(a) - (1-y) \\log(1-a)  \\tag{$\\hat{y}$ = a }\n",
        "\\end{align*}\n",
        "\n",
        "Taking partial derivative with respect to $w_1$\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial}{\\partial w_1} \\mathcal{J}(\\hat{y}, y) =  \\frac{\\partial}{\\partial w_1} \\left[ - y \\log(a) - (1-y) \\log(1-a) \\right]\n",
        "\\end{align*}\n",
        "\n",
        "Using chain rule,\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{J}}{\\partial w_1}  =  \\frac{\\partial \\mathcal{J}}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z} {\\partial w_1}\n",
        "\\end{align*}\n",
        "\n",
        "Now figuring out the individual partial derivatives\n",
        "\\begin{align*}\n",
        " \\frac{\\partial \\mathcal{J}}{\\partial a} &= \\frac{\\partial}{\\partial a} \\left[ - y \\log(a) - (1-y) \\log(1-a) \\right] \\\\\n",
        " &= -y \\left( \\frac{1}{a} \\right) -(-1) \\left( \\frac{1-y}{1-a} \\right) \\\\\n",
        " & = \\frac{a-y}{a(1-a)}\n",
        "\\end{align*}\n",
        "\n",
        "Similarly,\n",
        "\\begin{align*}\n",
        "\\frac{\\partial a}{\\partial z} = a(1-a) \\tag{$a = \\sigma(z) = \\frac{1}{1+e^{-z}}$}\n",
        "\\end{align*}\n",
        "\n",
        "Lastly,\n",
        "$$\n",
        "\\frac{\\partial z} {\\partial w_1} = \\frac{\\partial (w_1 x_1 + w_2 x_2 + b)}{\\partial w_1} = x_1\n",
        "$$\n",
        "\n",
        "Hence,\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{J}}{\\partial w_1}  =  \\frac{\\partial \\mathcal{J}}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z} {\\partial w_1} = \\frac{a-y}{a(1-a)} a(1-a) x_1 = (a-y)x_1\n",
        "\\end{align*}\n",
        "\n",
        "Now, we can update $w_1$ using gradient descent\n",
        "$$\n",
        "w_1 = w_1 -\\alpha \\frac{\\partial \\mathcal{J}}{\\partial w_1}\n",
        "$$\n",
        "\n",
        "Similarly, for all parameters\n",
        "\\begin{align*}\n",
        "w_i &= w_i - \\alpha  \\frac{\\partial \\mathcal{J}}{\\partial w_i} \\tag{i=1,2,...,m; m is the number of parameters} \\\\\n",
        "b &= b - \\alpha  \\frac{\\partial \\mathcal{J}}{\\partial b}\n",
        "\\end{align*}\n"
      ],
      "metadata": {
        "id": "fdN4t2Vv_8o1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For m examples, Binary cross entropy loss is given as\n",
        "\n",
        "$$\n",
        " \\mathcal{J}(w) =- \\frac{1}{m} \\sum_{i=1}^m y_i \\log (\\sigma(w^T x_i)) + (1-y_i) \\log (1-\\sigma(w^Tx_i) )\n",
        "$$\n",
        "\n",
        "Taking partial derivative with respect to $w_j$\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial J(w)}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\left[ - \\frac{1}{m} \\sum_{i=1}^m y_i \\log (\\sigma(w^T x_i)) + (1-y_i) \\log (1-\\sigma(w^Tx_i) ) \\right]\n",
        "\\end{align*}\n",
        "\n",
        "Using chain rule, we get\n",
        "\\begin{align*}\n",
        "\\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial J(w)}{\\partial \\sigma (w^Tx_i) } \\frac{\\partial \\sigma (w^Tx_i) }{\\partial (w^T x_i)} \\frac{\\partial w^T x_i}{\\partial w_j} \\tag{i}\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial J(w)}{\\partial \\sigma(w^Tx_i)} &= \\frac{\\partial}{\\partial \\sigma (w^T x)} \\left[ - ( y_i \\log (\\sigma(w^T x_i)) + (1-y_i) \\log (1-\\sigma(w^Tx_i) )) \\right] \\\\\n",
        "&= - \\left[ \\frac{y_i}{\\sigma (w^T x)} - \\frac{(1-y_i)}{ 1- \\sigma (w^Tx)} \\right] \\\\\n",
        "&=  \\left[  \\frac{\\sigma(w^T x_i) -  y_i}{\\sigma(w^Tx_i) (1-\\sigma(w^T x_i)} \\right] \\tag{a}\n",
        "\\end{align*}\n",
        "\n",
        "and, we know $$ \\sigma(w^Tx) = \\frac{1}{1 + e^{-w^Tx}}$$\n",
        "Taking derivative with respect to $(w^T x)$ , we get\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\sigma (w^Tx_i) }{\\partial (w^T x_i)} = \\sigma(w^T x_i) ( 1 - \\sigma (w^T x_i) ) \\tag{b}\n",
        "\\end{align*}\n",
        "\n",
        "Lastly,\n",
        "\\begin{align*}\n",
        "\\frac{\\partial (w^Tx_i) }{\\partial w_j} = x^j \\tag{c}\n",
        "\\end{align*}\n",
        "\n",
        "Substituting (a), (b), and (c) in (i) we get\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial J(w)}{\\partial w_j} &= \\frac{1}{m} \\sum_{i=1}^m   \\frac{\\sigma(w^T x_i) -  y_i}{\\sigma(w^Tx_i) (1-\\sigma(w^T x_i)}  [\\sigma(w^T x_i) ( 1 - \\sigma (w^T x_i) ) ] x^j \\\\\n",
        "\\frac{\\partial J(w)}{\\partial w_j} &=  \\frac{1}{m} \\sum_{i=1}^m ( \\sigma(w^T x_i) - y_i) x^j\n",
        "\\end{align*}\n",
        "\n",
        "Now we can use  Gradient descent to optimize the $w$ parameters\n",
        "\n",
        "$$\n",
        " w_{j+1} = w_j - \\alpha \\frac{\\partial \\mathcal{J}(w)}{\\partial w_j} \\tag{Equation 4}\n",
        "$$\n",
        "\n",
        "Equation 4 is used to update the parameters $w$ based on the gradient. The amount of the movement in the gradient descent is given by the slope of the cost function weighted by a learning rate $\\alpha$.\n",
        "\n"
      ],
      "metadata": {
        "id": "3ee-Xn7cBt_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Takeaways\n",
        "- Sigmoid function estimates probabilities and hence can be used to make a prediction.\n",
        "- Binary Cross Entropy can be used as the cost function for the logistic regression classification.\n",
        "- Binary cross entropy has no closed-form solution but it is a convex function thus optimization methods such as gradient descent can be used to find the optimum parameters."
      ],
      "metadata": {
        "id": "lMiVdn9xLuBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "* Books\n",
        "  *  Aurélien Géron (2017), Hands-On Machine Learning with Scikit-Learn and TensorFlow, 1st edition, O'Reilly\n",
        "    * Part I, Chapter 4 Logistic Regression, page 202\n",
        "\n",
        "\n",
        "- [Cross entropy](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_2_Cross_entropy.html)\n",
        "- [Binary cross entropy and logistic regression](https://towardsdatascience.com/binary-cross-entropy-and-logistic-regression-bf7098e75559)"
      ],
      "metadata": {
        "id": "7G41Av__Esn6"
      }
    }
  ]
}