{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvx6Ar4c1pSe"
   },
   "source": [
    "# Gradient descent for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlKB5q4D1y10"
   },
   "source": [
    "Now that you have learned how Gradient Descent works, let's use it to find the parameters of Linear Regression. Let's fit the linear regression on the same [Advertisesment Dataset](https://www.statlearning.com/s/Advertising.csv), but this time using Gradient Descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReVUHCNHiGWL"
   },
   "source": [
    "Let's import the necessary libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AcqV3VsCGXpY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnBw7u4u_L_O"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 1795,
     "status": "ok",
     "timestamp": 1620133587058,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "Fx3xywro_Pv3",
    "outputId": "2aebed00-23a5-46a8-bab0-9637c6469715"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "1  230.1   37.8       69.2   22.1\n",
       "2   44.5   39.3       45.1   10.4\n",
       "3   17.2   45.9       69.3    9.3\n",
       "4  151.5   41.3       58.5   18.5\n",
       "5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"https://www.statlearning.com/s/Advertising.csv\" \n",
    "     \n",
    "\n",
    "# Read the CSV data from the link\n",
    "data_df = pd.read_csv(data_path,index_col=0)\n",
    "\n",
    "# Print out first 5 samples from the DataFrame\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce49736IcehF"
   },
   "source": [
    "This is a multiple Linear Regression problem with three independent variables: TV , radio and newspaper and one dependent variable: sales. There are 200 samples in the dataset *ie.* $n = 200$\n",
    "\n",
    "This multiple linear regression problem can be represented in matrix form as:\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = \\mathbf{X} \\boldsymbol{\\beta}$$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\hat{y_1} \\\\ \n",
    "\\hat{y_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y_{200}}\n",
    "\\end{bmatrix} =   \\begin{bmatrix}\n",
    "  1 & x_{1\\ 1} & x_{1\\ 2} & x_{1\\ 3} \\\\\n",
    "  1 & x_{2\\ 1} & x_{2\\ 2} & x_{2\\ 3} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  1 & x_{200\\ 1} & x_{200\\ 2} & x_{200\\ 3}\n",
    " \\end{bmatrix} \\times \\begin{bmatrix}\n",
    "\\beta_0 \\\\ \n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\beta_3\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "The predicted output for the samples can be computed as:\n",
    "\\begin{align*}\\hat{y_1} &= \\beta_0x_{1\\ 0}+ \\beta_1x_{1\\ 1} + \\beta_2x_{1\\ 2} + \\beta_3 x_{1\\ 3}\\\\\n",
    "\\hat{y_2} &= \\beta_0x_{2\\ 0}+ \\beta_2x_{2\\ 1} + \\beta_2x_{2\\ 2} + \\beta_3 x_{2\\ 3}\\\\\n",
    "\\hat{y_3} &= \\beta_0x_{3\\ 0}+ \\beta_1x_{3\\ 1} + \\beta_2x_{3\\ 2} + \\beta_3 x_{3\\ 3}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "\\hat{y_{200}} &= \\beta_0x_{200\\ 0}+ \\beta_1x_{200\\ 1} + \\beta_2x_{200\\ 2} + \\beta_3 x_{200\\ 3}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Generalizing, for any $i^{th}$ sample, predicted output can be computed as:\n",
    "\n",
    "$$\\hat{y_i} = \\beta_0x_{i\\ 0}+ \\beta_1x_{i\\ 1} + \\beta_2x_{i\\ 2} + \\beta_3 x_{i\\ 3}$$\n",
    " where for all $i$ = $1$ to $n$, $x_{i0} =1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5BWRU-N_Uz-"
   },
   "source": [
    "The following block of codes creates a matrix $\\mathbf{X}$ containing the features of all the samples and a vector $\\mathbf{y}$ containing their corresponding outputs. It also assigns the number of samples to $n$ and the number of features to $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Qbsl4vbTFk3i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1. , 230.1,  37.8,  69.2],\n",
       "       [  1. ,  44.5,  39.3,  45.1],\n",
       "       [  1. ,  17.2,  45.9,  69.3],\n",
       "       [  1. , 151.5,  41.3,  58.5],\n",
       "       [  1. , 180.8,  10.8,  58.4],\n",
       "       [  1. ,   8.7,  48.9,  75. ],\n",
       "       [  1. ,  57.5,  32.8,  23.5],\n",
       "       [  1. , 120.2,  19.6,  11.6],\n",
       "       [  1. ,   8.6,   2.1,   1. ],\n",
       "       [  1. , 199.8,   2.6,  21.2],\n",
       "       [  1. ,  66.1,   5.8,  24.2],\n",
       "       [  1. , 214.7,  24. ,   4. ],\n",
       "       [  1. ,  23.8,  35.1,  65.9],\n",
       "       [  1. ,  97.5,   7.6,   7.2],\n",
       "       [  1. , 204.1,  32.9,  46. ],\n",
       "       [  1. , 195.4,  47.7,  52.9],\n",
       "       [  1. ,  67.8,  36.6, 114. ],\n",
       "       [  1. , 281.4,  39.6,  55.8],\n",
       "       [  1. ,  69.2,  20.5,  18.3],\n",
       "       [  1. , 147.3,  23.9,  19.1],\n",
       "       [  1. , 218.4,  27.7,  53.4],\n",
       "       [  1. , 237.4,   5.1,  23.5],\n",
       "       [  1. ,  13.2,  15.9,  49.6],\n",
       "       [  1. , 228.3,  16.9,  26.2],\n",
       "       [  1. ,  62.3,  12.6,  18.3],\n",
       "       [  1. , 262.9,   3.5,  19.5],\n",
       "       [  1. , 142.9,  29.3,  12.6],\n",
       "       [  1. , 240.1,  16.7,  22.9],\n",
       "       [  1. , 248.8,  27.1,  22.9],\n",
       "       [  1. ,  70.6,  16. ,  40.8],\n",
       "       [  1. , 292.9,  28.3,  43.2],\n",
       "       [  1. , 112.9,  17.4,  38.6],\n",
       "       [  1. ,  97.2,   1.5,  30. ],\n",
       "       [  1. , 265.6,  20. ,   0.3],\n",
       "       [  1. ,  95.7,   1.4,   7.4],\n",
       "       [  1. , 290.7,   4.1,   8.5],\n",
       "       [  1. , 266.9,  43.8,   5. ],\n",
       "       [  1. ,  74.7,  49.4,  45.7],\n",
       "       [  1. ,  43.1,  26.7,  35.1],\n",
       "       [  1. , 228. ,  37.7,  32. ],\n",
       "       [  1. , 202.5,  22.3,  31.6],\n",
       "       [  1. , 177. ,  33.4,  38.7],\n",
       "       [  1. , 293.6,  27.7,   1.8],\n",
       "       [  1. , 206.9,   8.4,  26.4],\n",
       "       [  1. ,  25.1,  25.7,  43.3],\n",
       "       [  1. , 175.1,  22.5,  31.5],\n",
       "       [  1. ,  89.7,   9.9,  35.7],\n",
       "       [  1. , 239.9,  41.5,  18.5],\n",
       "       [  1. , 227.2,  15.8,  49.9],\n",
       "       [  1. ,  66.9,  11.7,  36.8],\n",
       "       [  1. , 199.8,   3.1,  34.6],\n",
       "       [  1. , 100.4,   9.6,   3.6],\n",
       "       [  1. , 216.4,  41.7,  39.6],\n",
       "       [  1. , 182.6,  46.2,  58.7],\n",
       "       [  1. , 262.7,  28.8,  15.9],\n",
       "       [  1. , 198.9,  49.4,  60. ],\n",
       "       [  1. ,   7.3,  28.1,  41.4],\n",
       "       [  1. , 136.2,  19.2,  16.6],\n",
       "       [  1. , 210.8,  49.6,  37.7],\n",
       "       [  1. , 210.7,  29.5,   9.3],\n",
       "       [  1. ,  53.5,   2. ,  21.4],\n",
       "       [  1. , 261.3,  42.7,  54.7],\n",
       "       [  1. , 239.3,  15.5,  27.3],\n",
       "       [  1. , 102.7,  29.6,   8.4],\n",
       "       [  1. , 131.1,  42.8,  28.9],\n",
       "       [  1. ,  69. ,   9.3,   0.9],\n",
       "       [  1. ,  31.5,  24.6,   2.2],\n",
       "       [  1. , 139.3,  14.5,  10.2],\n",
       "       [  1. , 237.4,  27.5,  11. ],\n",
       "       [  1. , 216.8,  43.9,  27.2],\n",
       "       [  1. , 199.1,  30.6,  38.7],\n",
       "       [  1. , 109.8,  14.3,  31.7],\n",
       "       [  1. ,  26.8,  33. ,  19.3],\n",
       "       [  1. , 129.4,   5.7,  31.3],\n",
       "       [  1. , 213.4,  24.6,  13.1],\n",
       "       [  1. ,  16.9,  43.7,  89.4],\n",
       "       [  1. ,  27.5,   1.6,  20.7],\n",
       "       [  1. , 120.5,  28.5,  14.2],\n",
       "       [  1. ,   5.4,  29.9,   9.4],\n",
       "       [  1. , 116. ,   7.7,  23.1],\n",
       "       [  1. ,  76.4,  26.7,  22.3],\n",
       "       [  1. , 239.8,   4.1,  36.9],\n",
       "       [  1. ,  75.3,  20.3,  32.5],\n",
       "       [  1. ,  68.4,  44.5,  35.6],\n",
       "       [  1. , 213.5,  43. ,  33.8],\n",
       "       [  1. , 193.2,  18.4,  65.7],\n",
       "       [  1. ,  76.3,  27.5,  16. ],\n",
       "       [  1. , 110.7,  40.6,  63.2],\n",
       "       [  1. ,  88.3,  25.5,  73.4],\n",
       "       [  1. , 109.8,  47.8,  51.4],\n",
       "       [  1. , 134.3,   4.9,   9.3],\n",
       "       [  1. ,  28.6,   1.5,  33. ],\n",
       "       [  1. , 217.7,  33.5,  59. ],\n",
       "       [  1. , 250.9,  36.5,  72.3],\n",
       "       [  1. , 107.4,  14. ,  10.9],\n",
       "       [  1. , 163.3,  31.6,  52.9],\n",
       "       [  1. , 197.6,   3.5,   5.9],\n",
       "       [  1. , 184.9,  21. ,  22. ],\n",
       "       [  1. , 289.7,  42.3,  51.2],\n",
       "       [  1. , 135.2,  41.7,  45.9],\n",
       "       [  1. , 222.4,   4.3,  49.8],\n",
       "       [  1. , 296.4,  36.3, 100.9],\n",
       "       [  1. , 280.2,  10.1,  21.4],\n",
       "       [  1. , 187.9,  17.2,  17.9],\n",
       "       [  1. , 238.2,  34.3,   5.3],\n",
       "       [  1. , 137.9,  46.4,  59. ],\n",
       "       [  1. ,  25. ,  11. ,  29.7],\n",
       "       [  1. ,  90.4,   0.3,  23.2],\n",
       "       [  1. ,  13.1,   0.4,  25.6],\n",
       "       [  1. , 255.4,  26.9,   5.5],\n",
       "       [  1. , 225.8,   8.2,  56.5],\n",
       "       [  1. , 241.7,  38. ,  23.2],\n",
       "       [  1. , 175.7,  15.4,   2.4],\n",
       "       [  1. , 209.6,  20.6,  10.7],\n",
       "       [  1. ,  78.2,  46.8,  34.5],\n",
       "       [  1. ,  75.1,  35. ,  52.7],\n",
       "       [  1. , 139.2,  14.3,  25.6],\n",
       "       [  1. ,  76.4,   0.8,  14.8],\n",
       "       [  1. , 125.7,  36.9,  79.2],\n",
       "       [  1. ,  19.4,  16. ,  22.3],\n",
       "       [  1. , 141.3,  26.8,  46.2],\n",
       "       [  1. ,  18.8,  21.7,  50.4],\n",
       "       [  1. , 224. ,   2.4,  15.6],\n",
       "       [  1. , 123.1,  34.6,  12.4],\n",
       "       [  1. , 229.5,  32.3,  74.2],\n",
       "       [  1. ,  87.2,  11.8,  25.9],\n",
       "       [  1. ,   7.8,  38.9,  50.6],\n",
       "       [  1. ,  80.2,   0. ,   9.2],\n",
       "       [  1. , 220.3,  49. ,   3.2],\n",
       "       [  1. ,  59.6,  12. ,  43.1],\n",
       "       [  1. ,   0.7,  39.6,   8.7],\n",
       "       [  1. , 265.2,   2.9,  43. ],\n",
       "       [  1. ,   8.4,  27.2,   2.1],\n",
       "       [  1. , 219.8,  33.5,  45.1],\n",
       "       [  1. ,  36.9,  38.6,  65.6],\n",
       "       [  1. ,  48.3,  47. ,   8.5],\n",
       "       [  1. ,  25.6,  39. ,   9.3],\n",
       "       [  1. , 273.7,  28.9,  59.7],\n",
       "       [  1. ,  43. ,  25.9,  20.5],\n",
       "       [  1. , 184.9,  43.9,   1.7],\n",
       "       [  1. ,  73.4,  17. ,  12.9],\n",
       "       [  1. , 193.7,  35.4,  75.6],\n",
       "       [  1. , 220.5,  33.2,  37.9],\n",
       "       [  1. , 104.6,   5.7,  34.4],\n",
       "       [  1. ,  96.2,  14.8,  38.9],\n",
       "       [  1. , 140.3,   1.9,   9. ],\n",
       "       [  1. , 240.1,   7.3,   8.7],\n",
       "       [  1. , 243.2,  49. ,  44.3],\n",
       "       [  1. ,  38. ,  40.3,  11.9],\n",
       "       [  1. ,  44.7,  25.8,  20.6],\n",
       "       [  1. , 280.7,  13.9,  37. ],\n",
       "       [  1. , 121. ,   8.4,  48.7],\n",
       "       [  1. , 197.6,  23.3,  14.2],\n",
       "       [  1. , 171.3,  39.7,  37.7],\n",
       "       [  1. , 187.8,  21.1,   9.5],\n",
       "       [  1. ,   4.1,  11.6,   5.7],\n",
       "       [  1. ,  93.9,  43.5,  50.5],\n",
       "       [  1. , 149.8,   1.3,  24.3],\n",
       "       [  1. ,  11.7,  36.9,  45.2],\n",
       "       [  1. , 131.7,  18.4,  34.6],\n",
       "       [  1. , 172.5,  18.1,  30.7],\n",
       "       [  1. ,  85.7,  35.8,  49.3],\n",
       "       [  1. , 188.4,  18.1,  25.6],\n",
       "       [  1. , 163.5,  36.8,   7.4],\n",
       "       [  1. , 117.2,  14.7,   5.4],\n",
       "       [  1. , 234.5,   3.4,  84.8],\n",
       "       [  1. ,  17.9,  37.6,  21.6],\n",
       "       [  1. , 206.8,   5.2,  19.4],\n",
       "       [  1. , 215.4,  23.6,  57.6],\n",
       "       [  1. , 284.3,  10.6,   6.4],\n",
       "       [  1. ,  50. ,  11.6,  18.4],\n",
       "       [  1. , 164.5,  20.9,  47.4],\n",
       "       [  1. ,  19.6,  20.1,  17. ],\n",
       "       [  1. , 168.4,   7.1,  12.8],\n",
       "       [  1. , 222.4,   3.4,  13.1],\n",
       "       [  1. , 276.9,  48.9,  41.8],\n",
       "       [  1. , 248.4,  30.2,  20.3],\n",
       "       [  1. , 170.2,   7.8,  35.2],\n",
       "       [  1. , 276.7,   2.3,  23.7],\n",
       "       [  1. , 165.6,  10. ,  17.6],\n",
       "       [  1. , 156.6,   2.6,   8.3],\n",
       "       [  1. , 218.5,   5.4,  27.4],\n",
       "       [  1. ,  56.2,   5.7,  29.7],\n",
       "       [  1. , 287.6,  43. ,  71.8],\n",
       "       [  1. , 253.8,  21.3,  30. ],\n",
       "       [  1. , 205. ,  45.1,  19.6],\n",
       "       [  1. , 139.5,   2.1,  26.6],\n",
       "       [  1. , 191.1,  28.7,  18.2],\n",
       "       [  1. , 286. ,  13.9,   3.7],\n",
       "       [  1. ,  18.7,  12.1,  23.4],\n",
       "       [  1. ,  39.5,  41.1,   5.8],\n",
       "       [  1. ,  75.5,  10.8,   6. ],\n",
       "       [  1. ,  17.2,   4.1,  31.6],\n",
       "       [  1. , 166.8,  42. ,   3.6],\n",
       "       [  1. , 149.7,  35.6,   6. ],\n",
       "       [  1. ,  38.2,   3.7,  13.8],\n",
       "       [  1. ,  94.2,   4.9,   8.1],\n",
       "       [  1. , 177. ,   9.3,   6.4],\n",
       "       [  1. , 283.6,  42. ,  66.2],\n",
       "       [  1. , 232.1,   8.6,   8.7]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature matrix\n",
    "X = data_df[['TV', 'radio', 'newspaper']].values\n",
    "\n",
    "# Adding the column of ones in X\n",
    "X = np.c_[np.ones((X.shape[0],1)),X]\n",
    "\n",
    "# Outputs\n",
    "y = data_df['sales'].values.reshape(-1,1)\n",
    "\n",
    "n = X.shape[0] # number of samples (rows)\n",
    "d = X.shape[1] # number of features (columns)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PucmdL0EtSaM"
   },
   "source": [
    "## Random Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nt6JrDuKidI"
   },
   "source": [
    "Let's initialize the values of parameters randomly. The function `initialize_beta` uses the [`numpy.random.randn`](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html) function to initialize the parameters using the random values sampled from **standard normal distribution**. It returns an array of the shape $d\\times 1$ (where $d$ = no. of features) containing the initial values of the parameters. In our case $d=4$ (including the ones column).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1620133587060,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "abLGKrPft2JS",
    "outputId": "a1b83393-7e6a-4976-d12c-5d45806de863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.76405235]\n",
      " [0.40015721]\n",
      " [0.97873798]\n",
      " [2.2408932 ]]\n"
     ]
    }
   ],
   "source": [
    "def initialize_betas(X, y):\n",
    "  np.random.seed(0)\n",
    "  betas = np.random.randn(d,1)\n",
    "  return betas\n",
    "\n",
    "betas = initialize_betas(X, y)\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKnXrYVtOxI0"
   },
   "source": [
    "Here, the initial values for our parameters are:\n",
    "\n",
    "$$\\boldsymbol{\\beta} =\\begin{bmatrix}\n",
    "\\beta_0 \\\\ \n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\beta_3\n",
    "\\end{bmatrix} =   \\begin{bmatrix}\n",
    "  1.7640\\\\\n",
    "  0.4001\\\\\n",
    "  0.9787\\\\\n",
    "  2.2408\n",
    " \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOW6cAC9_kBG"
   },
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhcPRsmcvQRG"
   },
   "source": [
    "In OLS, you minimized the sum of squared error (SSE). Here you will be minimizing the cost function. The cost function $J(.)$ is nothing but the sum of squared error multiplied by $\\frac{1}{2}$ to make the derivation easier. You should know that multiplying the cost function with $\\frac{1}{2}$ only changes the value of the cost function but not the optimal parameters that minimize it.\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta_0, \\beta_1, \\beta_2, \\beta_3) &= \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})^2 \\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^{n}((\\beta_0x_{i0}+\\beta_1x_{i1} +\\beta_2x_{i2} + \\beta_3x_{i3})-y_{i})^2\n",
    "\\end{align*}\n",
    "\n",
    "The cost function can be written in matrix form as:\n",
    "\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2}\\ \\sum(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})^2$$\n",
    "\n",
    "\n",
    "*Note: We call $J$ as a function of only parameters $\\boldsymbol{\\beta}$ but not of $X$ and $y$ because $X$ and $y$ are constants given by the dataset. So the value of $J$ depends only on the parameters.*\n",
    "\n",
    "**You want to find the parameters $\\beta_0, \\beta_1, \\beta_2$ and $\\beta_3$ that minimizes the cost function $J$ using Gradient Descent.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liAx8tsai2No"
   },
   "source": [
    "The `calculate_cost` function below calculates the cost function for a particular set of values of parameters `betas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1763,
     "status": "ok",
     "timestamp": 1620133587062,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "s4OEEDOSFdco",
    "outputId": "f30a2ad6-4757-47ac-cfe1-796221a4768a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost with random betas: 2303714.226243876\n"
     ]
    }
   ],
   "source": [
    "def calculate_cost(betas):\n",
    "  cost = 1/2 * np.sum(np.square(np.dot(X, betas)-y))\n",
    "  return cost\n",
    "\n",
    "print(\"Cost with random betas:\", calculate_cost(betas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbvmnpbKj0wk"
   },
   "source": [
    "## Gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G07bkOBORc5Y"
   },
   "source": [
    "You will need to calculate the gradient of the cost function with respect to each of the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxNS4h6TobSc"
   },
   "source": [
    "Partial derivative(gradient) of the cost function with respect to $\\beta_1$, \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\beta_1} &= \\frac{\\partial}{\\partial \\beta_1}\\ \\frac{1}{2}\\ \\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})^2\\\\\n",
    "&=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial}{\\partial \\beta_1}(\\hat{y_{i}}-{y_{i}})^2\n",
    "\\end{align*}\n",
    "\n",
    "$\\hspace{8cm}$Applying chain rule,\n",
    "\n",
    "\\begin{align*}\n",
    "\\hspace{8cm}&=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial (\\hat{y_{i}}-{y_{i}})^2}{\\partial (\\hat{y_{i}}-{y_{i}})} \\times \\frac{\\partial (\\hat{y_{i}}-{y_{i}})}{\\partial \\beta_1}\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i}) \\times \\frac{\\partial (\\beta_0x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3})-y_i)}{\\partial \\beta_1}\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i}) \\times x_{i1}\\\\\n",
    "\\therefore \\frac{\\partial J}{\\partial \\beta_1}&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i1} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MVtD6EAo_1g"
   },
   "source": [
    "Similarly, \n",
    "\n",
    "\\begin{align*}\\frac{\\partial J}{\\partial \\beta_0}&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i0}\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})\\times 1\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_2}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i2}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_3}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i3}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thYWZqkPSQ85"
   },
   "source": [
    "In general, the formula for calculating the gradients with respect to a parameter $\\beta_j$ can be expressed as:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_j}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{ij}$$\n",
    "\n",
    "\n",
    "We can write this generalized expression in matrix form to calculate the gradients wrt. all the parameters simultaneously as:\n",
    "\n",
    "$$\\frac{\\boldsymbol{\\partial J}}{\\boldsymbol{\\partial \\beta}}= \\mathbf{X^T}(\\mathbf{\\hat{y}-y}) = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\beta_0} \\\\ \n",
    "\\frac{\\partial J}{\\partial \\beta_1}\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_2}\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_3}\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Xz0hZVjaMr"
   },
   "source": [
    "The `calculate_gradient` function below calculates the gradients of cost function with respect to the parameters `betas`. It uses the matrix operations to compute the gradient of all the parameters simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1749,
     "status": "ok",
     "timestamp": 1620133587063,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "VGTQFH696hyd",
    "outputId": "86663aab-87c8-4b1f-a297-c672af6466e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for random betas = \n",
      " [[  27563.85598559]\n",
      " [4631129.37253468]\n",
      " [ 731917.2683292 ]\n",
      " [1079270.65268036]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_gradients(betas):\n",
    "  gradients = np.dot(X.T,(np.dot(X,betas)-y))\n",
    "  return gradients\n",
    "\n",
    "print(\"Gradients for random betas = \\n\", calculate_gradients(betas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJigUKetTBqB"
   },
   "source": [
    "Here, the gradients of the cost function with respect to the initial parameters are:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial J}{\\partial \\beta}}  =\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\beta_0} \\\\ \n",
    "\\frac{\\partial J}{\\partial \\beta_1}\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_2}\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_3}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "27563.85 \\\\ \n",
    "4631129.37\\\\\n",
    "731917.26\\\\\n",
    "1079270.65\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_wG80YPmuZG"
   },
   "source": [
    "You can see that the gradients for different parameters vary quite largely. The parameters corresponding to the features having larger values have larger gradients and the parameters corresponding to the features having smaller values have smaller gradients. For example, the gradients with respect to the parameter corresponding to the feature 'TV', $\\frac{\\partial J}{\\partial \\beta_1}$ is larger than the ones corresponing to the 'radio', $\\frac{\\partial J}{\\partial \\beta_2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcyFQ69asVOs"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tVDczBZUNUw"
   },
   "source": [
    "Now you need to update the parameters using their respective gradients until the cost function converges to its minimum value.\n",
    "\n",
    "\n",
    "${\\hspace{5cm}}\\text{Repeat until convergence }\\{$\n",
    "\n",
    "$$\\beta_0 :=\\beta_0-\\alpha\\frac{\\partial J}{\\partial \\beta_0}$$\n",
    "\n",
    "$$\\beta_1 :=\\beta_1-\\alpha\\frac{\\partial J}{\\partial \\beta_1}$$\n",
    "\n",
    "$$\\beta_2 :=\\beta_2-\\alpha\\frac{\\partial J}{\\partial \\beta_2}$$\n",
    "\n",
    "$$\\beta_3 :=\\beta_3-\\alpha\\frac{\\partial J}{\\partial \\beta_3}$$\n",
    "\n",
    "${\\hspace{8cm}}\\}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since you already have a vector $\\beta$ called `beta` containing parameters and an another vector $\\frac{\\partial J}{\\partial \\beta}$ called `gradients` containing the gradients of cost function with respect to the parameters, this updation is a simple matrix operation:\n",
    "\n",
    "$$\\boldsymbol{\\beta} := \\boldsymbol{\\beta} - \\alpha \\boldsymbol{\\frac{\\partial J}{\\partial \\beta}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adoBYUlCk38k"
   },
   "source": [
    "The `gradient_descent` function below applies the gradient descent algorithm to find the optimal parameters `betas` that minimize the cost function. Initially `betas` contain the random initial values of the parameters. It uses the gradients calculated by the `calculate_gradients` function to update the values of the parameters. The default learning rate is set to `alpha=0.003`. The process of updating the parameters is repeated till the cost function decreases by a certain threshold value `precision` or the maximum number of iterations `max_iters` is not reached. Also the list `costs`  contains the values of cost functions for different values of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZVha43ihsdqB"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha=0.003 , max_iters=10000, precision = 1e-3):\n",
    "  iteration = 0 # no. of iterations\n",
    "  difference = 1\n",
    "  betas = initialize_betas(X,y) # random initial values of the parameters\n",
    "  cost = calculate_cost(betas) # cost for the initial values pf parameters\n",
    "  costs = [calculate_cost(betas)] # list containing the history of costs for different iterations\n",
    "\n",
    "  while difference > precision and iteration <= max_iters :\n",
    "    # updating the values of parameters\n",
    "    betas = betas - alpha * calculate_gradients(betas)\n",
    "\n",
    "    # cost for the new values of parameters\n",
    "    cost = calculate_cost(betas)\n",
    "\n",
    "    # difference between the cost of current iteration and previous iteration\n",
    "    difference = np.abs(costs[iteration] - cost) \n",
    "    costs.append(cost)\n",
    "    \n",
    "    print(\"iteration: {}, cost: {}\".format(iteration, cost))\n",
    "    iteration += 1\n",
    "    \n",
    "    if(cost == np.infty):\n",
    "      print(\"Cost reached infinity, try smaller learning rate\")\n",
    "      break\n",
    "    \n",
    "  return betas, iteration, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQhroBc1rvzi"
   },
   "source": [
    "Let's use the `gradient_descent` function defined above to learn the parameters for our multiple linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2800,
     "status": "ok",
     "timestamp": 1620133588132,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "M7rZkEHg0jga",
    "outputId": "c5c8da0b-96dc-4187-9b3a-5421df6036b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, cost: 624562462609042.9\n",
      "iteration: 1, cost: 2.0418880086534106e+23\n",
      "iteration: 2, cost: 6.676506400718878e+31\n",
      "iteration: 3, cost: 2.1830649810113797e+40\n",
      "iteration: 4, cost: 7.13812348195547e+48\n",
      "iteration: 5, cost: 2.3340032150596292e+57\n",
      "iteration: 6, cost: 7.6316570057658e+65\n",
      "iteration: 7, cost: 2.49537739613466e+74\n",
      "iteration: 8, cost: 8.159313690900017e+82\n",
      "iteration: 9, cost: 2.6679090709738826e+91\n",
      "iteration: 10, cost: 8.723452830258335e+99\n",
      "iteration: 11, cost: 2.852369674426849e+108\n",
      "iteration: 12, cost: 9.326596839463841e+116\n",
      "iteration: 13, cost: 3.0495839787448193e+125\n",
      "iteration: 14, cost: 9.971442535251373e+133\n",
      "iteration: 15, cost: 3.2604337813561266e+142\n",
      "iteration: 16, cost: 1.0660873193649934e+151\n",
      "iteration: 17, cost: 3.485861847616212e+159\n",
      "iteration: 18, cost: 1.1397971441874093e+168\n",
      "iteration: 19, cost: 3.7268761261614036e+176\n",
      "iteration: 20, cost: 1.2186033041567317e+185\n",
      "iteration: 21, cost: 3.984554255714459e+193\n",
      "iteration: 22, cost: 1.3028581625025874e+202\n",
      "iteration: 23, cost: 4.2600483834929084e+210\n",
      "iteration: 24, cost: 1.3929384450292768e+219\n",
      "iteration: 25, cost: 4.554590316764671e+227\n",
      "iteration: 26, cost: 1.4892469245567063e+236\n",
      "iteration: 27, cost: 4.86949703058485e+244\n",
      "iteration: 28, cost: 1.5922142218244213e+253\n",
      "iteration: 29, cost: 5.206176556340283e+261\n",
      "iteration: 30, cost: 1.7023007309110713e+270\n",
      "iteration: 31, cost: 5.566134277431072e+278\n",
      "iteration: 32, cost: 1.8199986777783747e+287\n",
      "iteration: 33, cost: 5.950979660238805e+295\n",
      "iteration: 34, cost: 1.945834321143854e+304\n",
      "iteration: 35, cost: inf\n",
      "Cost reached infinity, try smaller learning rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2660\\3834816075.py:2: RuntimeWarning: overflow encountered in square\n",
      "  cost = 1/2 * np.sum(np.square(np.dot(X, betas)-y))\n"
     ]
    }
   ],
   "source": [
    "betas, steps, costs = gradient_descent(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pCaStNn22KA"
   },
   "source": [
    "As you can see the default learning rate of `alpha=0.003` seems to be very large for this problem. The reason behind this is that the value of the gradients are very large. When the parameters are updated using these gradients, the parameters too get large values. After some iterations the cost function calculated using these parameters reach infinity.\n",
    "\n",
    "A possible solution to this is scaling the features to small values about which we will learn  in the next chapter. For now, let's further decrease the value of learning rate to `alpha=0.0000003`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2790,
     "status": "ok",
     "timestamp": 1620133588134,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "zSrmNc5s3xPJ",
    "outputId": "eec0b3df-74fa-4fb2-ad6c-46ff1346bc94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, cost: 1605439.1704912472\n",
      "iteration: 1, cost: 1139934.3901561853\n",
      "iteration: 2, cost: 827385.9388970851\n",
      "iteration: 3, cost: 615561.2099769667\n",
      "iteration: 4, cost: 470257.51679673104\n",
      "iteration: 5, cost: 369060.2077312675\n",
      "iteration: 6, cost: 297263.3981504862\n",
      "iteration: 7, cost: 245203.8822035737\n",
      "iteration: 8, cost: 206518.51314390055\n",
      "iteration: 9, cost: 177005.05468340195\n",
      "iteration: 10, cost: 153877.40255986896\n",
      "iteration: 11, cost: 135278.539277487\n",
      "iteration: 12, cost: 119961.93545091985\n",
      "iteration: 13, cost: 107083.05179937741\n",
      "iteration: 14, cost: 96062.81371070776\n",
      "iteration: 15, cost: 86498.14156564065\n",
      "iteration: 16, cost: 78103.252922825\n",
      "iteration: 17, cost: 70671.0939090806\n",
      "iteration: 18, cost: 64047.94353316392\n",
      "iteration: 19, cost: 58116.64362275174\n",
      "iteration: 20, cost: 52785.48135998221\n",
      "iteration: 21, cost: 47980.78022772064\n",
      "iteration: 22, cost: 43641.92759428901\n",
      "iteration: 23, cost: 39718.00667002997\n",
      "iteration: 24, cost: 36165.48787214866\n",
      "iteration: 25, cost: 32946.62247261573\n",
      "iteration: 26, cost: 30028.304239076748\n",
      "iteration: 27, cost: 27381.245129862684\n",
      "iteration: 28, cost: 24979.36368689\n",
      "iteration: 29, cost: 22799.319200830396\n",
      "iteration: 30, cost: 20820.14728572889\n",
      "iteration: 31, cost: 19022.96730223446\n",
      "iteration: 32, cost: 17390.741793970657\n",
      "iteration: 33, cost: 15908.07450475643\n",
      "iteration: 34, cost: 14561.03777225817\n",
      "iteration: 35, cost: 13337.022896030656\n",
      "iteration: 36, cost: 12224.608945120277\n",
      "iteration: 37, cost: 11213.446723202855\n",
      "iteration: 38, cost: 10294.155457505523\n",
      "iteration: 39, cost: 9458.230358953266\n",
      "iteration: 40, cost: 8697.95960514713\n",
      "iteration: 41, cost: 8006.349584004236\n",
      "iteration: 42, cost: 7377.057442986669\n",
      "iteration: 43, cost: 6804.330142372717\n",
      "iteration: 44, cost: 6282.949327852183\n",
      "iteration: 45, cost: 5808.181429040493\n",
      "iteration: 46, cost: 5375.732463753648\n",
      "iteration: 47, cost: 4981.707088065563\n",
      "iteration: 48, cost: 4622.5714826639905\n",
      "iteration: 49, cost: 4295.119709148255\n",
      "iteration: 50, cost: 3996.4432072822597\n",
      "iteration: 51, cost: 3723.903136970001\n",
      "iteration: 52, cost: 3475.1052976825404\n",
      "iteration: 53, cost: 3247.8773838461816\n",
      "iteration: 54, cost: 3040.2483577659596\n",
      "iteration: 55, cost: 2850.4297423695753\n",
      "iteration: 56, cost: 2676.798654705298\n",
      "iteration: 57, cost: 2517.8824179524026\n",
      "iteration: 58, cost: 2372.344604904434\n",
      "iteration: 59, cost: 2238.9723796353983\n",
      "iteration: 60, cost: 2116.6650165048686\n",
      "iteration: 61, cost: 2004.423486929932\n",
      "iteration: 62, cost: 1901.3410145647194\n",
      "iteration: 63, cost: 1806.5945087840637\n",
      "iteration: 64, cost: 1719.4367947581532\n",
      "iteration: 65, cost: 1639.1895660118457\n",
      "iteration: 66, cost: 1565.2369922596044\n",
      "iteration: 67, cost: 1497.0199215614025\n",
      "iteration: 68, cost: 1434.0306215166815\n",
      "iteration: 69, cost: 1375.8080093570693\n",
      "iteration: 70, cost: 1321.9333254633061\n",
      "iteration: 71, cost: 1272.0262090624208\n",
      "iteration: 72, cost: 1225.741138698099\n",
      "iteration: 73, cost: 1182.7642035470362\n",
      "iteration: 74, cost: 1142.8101748101642\n",
      "iteration: 75, cost: 1105.619849270082\n",
      "iteration: 76, cost: 1070.9576397021692\n",
      "iteration: 77, cost: 1038.6093891814758\n",
      "iteration: 78, cost: 1008.3803884630568\n",
      "iteration: 79, cost: 980.0935775503144\n",
      "iteration: 80, cost: 953.5879143226371\n",
      "iteration: 81, cost: 928.7168946869099\n",
      "iteration: 82, cost: 905.3472101625644\n",
      "iteration: 83, cost: 883.3575301204698\n",
      "iteration: 84, cost: 862.6373970847119\n",
      "iteration: 85, cost: 843.0862245844404\n",
      "iteration: 86, cost: 824.6123880208258\n",
      "iteration: 87, cost: 807.1323999010556\n",
      "iteration: 88, cost: 790.5701615956878\n",
      "iteration: 89, cost: 774.8562845052423\n",
      "iteration: 90, cost: 759.9274741836043\n",
      "iteration: 91, cost: 745.7259715659612\n",
      "iteration: 92, cost: 732.1990459933061\n",
      "iteration: 93, cost: 719.29853521923\n",
      "iteration: 94, cost: 706.9804280324779\n",
      "iteration: 95, cost: 695.2044855348569\n",
      "iteration: 96, cost: 683.9338974824184\n",
      "iteration: 97, cost: 673.1349704318984\n",
      "iteration: 98, cost: 662.7768447374043\n",
      "iteration: 99, cost: 652.8312377171362\n",
      "iteration: 100, cost: 643.2722105591884\n",
      "iteration: 101, cost: 634.0759567615321\n",
      "iteration: 102, cost: 625.2206101063143\n",
      "iteration: 103, cost: 616.686070354581\n",
      "iteration: 104, cost: 608.4538450161835\n",
      "iteration: 105, cost: 600.5069057026174\n",
      "iteration: 106, cost: 592.82955770928\n",
      "iteration: 107, cost: 585.407321599482\n",
      "iteration: 108, cost: 578.2268256766806\n",
      "iteration: 109, cost: 571.2757083349281\n",
      "iteration: 110, cost: 564.5425293714144\n",
      "iteration: 111, cost: 558.0166894301483\n",
      "iteration: 112, cost: 551.6883568230576\n",
      "iteration: 113, cost: 545.5484010448432\n",
      "iteration: 114, cost: 539.5883323614621\n",
      "iteration: 115, cost: 533.8002469097432\n",
      "iteration: 116, cost: 528.176776797912\n",
      "iteration: 117, cost: 522.7110447442067\n",
      "iteration: 118, cost: 517.3966228337687\n",
      "iteration: 119, cost: 512.227495012994\n",
      "iteration: 120, cost: 507.19802297590047\n",
      "iteration: 121, cost: 502.3029151291563\n",
      "iteration: 122, cost: 497.5371983515144\n",
      "iteration: 123, cost: 492.89619228978785\n",
      "iteration: 124, cost: 488.3754859574503\n",
      "iteration: 125, cost: 483.97091642365103\n",
      "iteration: 126, cost: 479.67854940013876\n",
      "iteration: 127, cost: 475.4946615514435\n",
      "iteration: 128, cost: 471.41572436987764\n",
      "iteration: 129, cost: 467.43838947160924\n",
      "iteration: 130, cost: 463.55947518339326\n",
      "iteration: 131, cost: 459.7759543016392\n",
      "iteration: 132, cost: 456.0849429164578\n",
      "iteration: 133, cost: 452.48369020328334\n",
      "iteration: 134, cost: 448.9695690936858\n",
      "iteration: 135, cost: 445.5400677451779\n",
      "iteration: 136, cost: 442.1927817372423\n",
      "iteration: 137, cost: 438.9254069275408\n",
      "iteration: 138, cost: 435.7357329083771\n",
      "iteration: 139, cost: 432.6216370090244\n",
      "iteration: 140, cost: 429.58107879455815\n",
      "iteration: 141, cost: 426.61209501638973\n",
      "iteration: 142, cost: 423.71279497384\n",
      "iteration: 143, cost: 420.8813562498376\n",
      "iteration: 144, cost: 418.1160207872335\n",
      "iteration: 145, cost: 415.4150912753129\n",
      "iteration: 146, cost: 412.7769278188824\n",
      "iteration: 147, cost: 410.19994486485814\n",
      "iteration: 148, cost: 407.68260836357786\n",
      "iteration: 149, cost: 405.2234331441616\n",
      "iteration: 150, cost: 402.82098048513365\n",
      "iteration: 151, cost: 400.473855863248\n",
      "iteration: 152, cost: 398.1807068650161\n",
      "iteration: 153, cost: 395.94022124685785\n",
      "iteration: 154, cost: 393.7511251310782\n",
      "iteration: 155, cost: 391.6121813260438\n",
      "iteration: 156, cost: 389.5221877599879\n",
      "iteration: 157, cost: 387.47997601883725\n",
      "iteration: 158, cost: 385.48440997932425\n",
      "iteration: 159, cost: 383.5343845294393\n",
      "iteration: 160, cost: 381.62882436899736\n",
      "iteration: 161, cost: 379.766682883745\n",
      "iteration: 162, cost: 377.94694108702356\n",
      "iteration: 163, cost: 376.16860662354543\n",
      "iteration: 164, cost: 374.43071283032265\n",
      "iteration: 165, cost: 372.73231785023654\n",
      "iteration: 166, cost: 371.0725037941301\n",
      "iteration: 167, cost: 369.450375947681\n",
      "iteration: 168, cost: 367.865062019632\n",
      "iteration: 169, cost: 366.31571142826823\n",
      "iteration: 170, cost: 364.8014946232963\n",
      "iteration: 171, cost: 363.32160244052955\n",
      "iteration: 172, cost: 361.8752454870146\n",
      "iteration: 173, cost: 360.4616535544325\n",
      "iteration: 174, cost: 359.0800750587965\n",
      "iteration: 175, cost: 357.72977650464213\n",
      "iteration: 176, cost: 356.4100419720486\n",
      "iteration: 177, cost: 355.12017262498426\n",
      "iteration: 178, cost: 353.859486239582\n",
      "iteration: 179, cost: 352.62731675107716\n",
      "iteration: 180, cost: 351.4230138182388\n",
      "iteration: 181, cost: 350.24594240422493\n",
      "iteration: 182, cost: 349.0954823728764\n",
      "iteration: 183, cost: 347.97102809954504\n",
      "iteration: 184, cost: 346.87198809562494\n",
      "iteration: 185, cost: 345.79778464601736\n",
      "iteration: 186, cost: 344.7478534588255\n",
      "iteration: 187, cost: 343.7216433266236\n",
      "iteration: 188, cost: 342.7186157987006\n",
      "iteration: 189, cost: 341.73824486371893\n",
      "iteration: 190, cost: 340.78001664227315\n",
      "iteration: 191, cost: 339.84342908887197\n",
      "iteration: 192, cost: 338.92799170289635\n",
      "iteration: 193, cost: 338.03322524812575\n",
      "iteration: 194, cost: 337.1586614804455\n",
      "iteration: 195, cost: 336.3038428833818\n",
      "iteration: 196, cost: 335.4683224111271\n",
      "iteration: 197, cost: 334.65166323874973\n",
      "iteration: 198, cost: 333.85343851929315\n",
      "iteration: 199, cost: 333.07323114749533\n",
      "iteration: 200, cost: 332.3106335298715\n",
      "iteration: 201, cost: 331.5652473609215\n",
      "iteration: 202, cost: 330.83668340523593\n",
      "iteration: 203, cost: 330.1245612852904\n",
      "iteration: 204, cost: 329.42850927472546\n",
      "iteration: 205, cost: 328.74816409692573\n",
      "iteration: 206, cost: 328.08317072871984\n",
      "iteration: 207, cost: 327.4331822090294\n",
      "iteration: 208, cost: 326.7978594523105\n",
      "iteration: 209, cost: 326.176871066633\n",
      "iteration: 210, cost: 325.5698931762546\n",
      "iteration: 211, cost: 324.9766092485517\n",
      "iteration: 212, cost: 324.3967099251745\n",
      "iteration: 213, cost: 323.829892857305\n",
      "iteration: 214, cost: 323.27586254489336\n",
      "iteration: 215, cost: 322.73433017976254\n",
      "iteration: 216, cost: 322.20501349247013\n",
      "iteration: 217, cost: 321.68763660282195\n",
      "iteration: 218, cost: 321.1819298739372\n",
      "iteration: 219, cost: 320.68762976976893\n",
      "iteration: 220, cost: 320.20447871598697\n",
      "iteration: 221, cost: 319.7322249641321\n",
      "iteration: 222, cost: 319.27062245895735\n",
      "iteration: 223, cost: 318.81943070887235\n",
      "iteration: 224, cost: 318.37841465941085\n",
      "iteration: 225, cost: 317.9473445696426\n",
      "iteration: 226, cost: 317.52599589145814\n",
      "iteration: 227, cost: 317.1141491516493\n",
      "iteration: 228, cost: 316.7115898367205\n",
      "iteration: 229, cost: 316.31810828035873\n",
      "iteration: 230, cost: 315.9334995534993\n",
      "iteration: 231, cost: 315.5575633569223\n",
      "iteration: 232, cost: 315.19010391631866\n",
      "iteration: 233, cost: 314.8309298797652\n",
      "iteration: 234, cost: 314.47985421755175\n",
      "iteration: 235, cost: 314.1366941243017\n",
      "iteration: 236, cost: 313.8012709233334\n",
      "iteration: 237, cost: 313.47340997320697\n",
      "iteration: 238, cost: 313.1529405764061\n",
      "iteration: 239, cost: 312.83969589010394\n",
      "iteration: 240, cost: 312.53351283896274\n",
      "iteration: 241, cost: 312.23423202992103\n",
      "iteration: 242, cost: 311.9416976689215\n",
      "iteration: 243, cost: 311.6557574795324\n",
      "iteration: 244, cost: 311.376262623421\n",
      "iteration: 245, cost: 311.103067622634\n",
      "iteration: 246, cost: 310.83603028364354\n",
      "iteration: 247, cost: 310.57501162311826\n",
      "iteration: 248, cost: 310.3198757953786\n",
      "iteration: 249, cost: 310.0704900214985\n",
      "iteration: 250, cost: 309.8267245200151\n",
      "iteration: 251, cost: 309.58845243920894\n",
      "iteration: 252, cost: 309.3555497909189\n",
      "iteration: 253, cost: 309.1278953858573\n",
      "iteration: 254, cost: 308.90537077039\n",
      "iteration: 255, cost: 308.6878601647477\n",
      "iteration: 256, cost: 308.4752504026362\n",
      "iteration: 257, cost: 308.2674308722145\n",
      "iteration: 258, cost: 308.06429345840723\n",
      "iteration: 259, cost: 307.86573248652286\n",
      "iteration: 260, cost: 307.67164466714786\n",
      "iteration: 261, cost: 307.48192904228614\n",
      "iteration: 262, cost: 307.2964869327176\n",
      "iteration: 263, cost: 307.11522188654607\n",
      "iteration: 264, cost: 306.9380396289113\n",
      "iteration: 265, cost: 306.7648480128369\n",
      "iteration: 266, cost: 306.5955569711899\n",
      "iteration: 267, cost: 306.43007846972534\n",
      "iteration: 268, cost: 306.268326461193\n",
      "iteration: 269, cost: 306.11021684048\n",
      "iteration: 270, cost: 305.9556674007677\n",
      "iteration: 271, cost: 305.8045977906787\n",
      "iteration: 272, cost: 305.6569294723921\n",
      "iteration: 273, cost: 305.51258568070494\n",
      "iteration: 274, cost: 305.37149138301766\n",
      "iteration: 275, cost: 305.2335732402249\n",
      "iteration: 276, cost: 305.0987595684878\n",
      "iteration: 277, cost: 304.9669803018707\n",
      "iteration: 278, cost: 304.83816695582135\n",
      "iteration: 279, cost: 304.71225259147565\n",
      "iteration: 280, cost: 304.589171780769\n",
      "iteration: 281, cost: 304.4688605723345\n",
      "iteration: 282, cost: 304.35125645817163\n",
      "iteration: 283, cost: 304.2362983410683\n",
      "iteration: 284, cost: 304.1239265027564\n",
      "iteration: 285, cost: 304.0140825727888\n",
      "iteration: 286, cost: 303.90670949811727\n",
      "iteration: 287, cost: 303.8017515133573\n",
      "iteration: 288, cost: 303.6991541117257\n",
      "iteration: 289, cost: 303.59886401663175\n",
      "iteration: 290, cost: 303.5008291539129\n",
      "iteration: 291, cost: 303.404998624694\n",
      "iteration: 292, cost: 303.3113226788612\n",
      "iteration: 293, cost: 303.2197526891348\n",
      "iteration: 294, cost: 303.13024112572566\n",
      "iteration: 295, cost: 303.0427415315651\n",
      "iteration: 296, cost: 302.9572084980939\n",
      "iteration: 297, cost: 302.873597641597\n",
      "iteration: 298, cost: 302.79186558007325\n",
      "iteration: 299, cost: 302.7119699106263\n",
      "iteration: 300, cost: 302.63386918736694\n",
      "iteration: 301, cost: 302.5575228998134\n",
      "iteration: 302, cost: 302.4828914517794\n",
      "iteration: 303, cost: 302.4099361407385\n",
      "iteration: 304, cost: 302.3386191376553\n",
      "iteration: 305, cost: 302.2689034672703\n",
      "iteration: 306, cost: 302.2007529888308\n",
      "iteration: 307, cost: 302.13413237725615\n",
      "iteration: 308, cost: 302.06900710472877\n",
      "iteration: 309, cost: 302.0053434227003\n",
      "iteration: 310, cost: 301.94310834430337\n",
      "iteration: 311, cost: 301.88226962716135\n",
      "iteration: 312, cost: 301.82279575658515\n",
      "iteration: 313, cost: 301.76465592915025\n",
      "iteration: 314, cost: 301.7078200366434\n",
      "iteration: 315, cost: 301.6522586503719\n",
      "iteration: 316, cost: 301.5979430058282\n",
      "iteration: 317, cost: 301.54484498769835\n",
      "iteration: 318, cost: 301.49293711521113\n",
      "iteration: 319, cost: 301.442192527816\n",
      "iteration: 320, cost: 301.39258497118624\n",
      "iteration: 321, cost: 301.3440887835351\n",
      "iteration: 322, cost: 301.29667888224435\n",
      "iteration: 323, cost: 301.2503307507918\n",
      "iteration: 324, cost: 301.20502042597525\n",
      "iteration: 325, cost: 301.16072448542405\n",
      "iteration: 326, cost: 301.1174200353934\n",
      "iteration: 327, cost: 301.07508469883345\n",
      "iteration: 328, cost: 301.0336966037275\n",
      "iteration: 329, cost: 300.9932343716942\n",
      "iteration: 330, cost: 300.9536771068466\n",
      "iteration: 331, cost: 300.9150043849018\n",
      "iteration: 332, cost: 300.8771962425384\n",
      "iteration: 333, cost: 300.84023316699177\n",
      "iteration: 334, cost: 300.804096085886\n",
      "iteration: 335, cost: 300.76876635729445\n",
      "iteration: 336, cost: 300.7342257600249\n",
      "iteration: 337, cost: 300.70045648412355\n",
      "iteration: 338, cost: 300.66744112159415\n",
      "iteration: 339, cost: 300.63516265732574\n",
      "iteration: 340, cost: 300.6036044602256\n",
      "iteration: 341, cost: 300.57275027455245\n",
      "iteration: 342, cost: 300.5425842114447\n",
      "iteration: 343, cost: 300.51309074064045\n",
      "iteration: 344, cost: 300.4842546823843\n",
      "iteration: 345, cost: 300.4560611995163\n",
      "iteration: 346, cost: 300.42849578973994\n",
      "iteration: 347, cost: 300.40154427806505\n",
      "iteration: 348, cost: 300.37519280941984\n",
      "iteration: 349, cost: 300.3494278414313\n",
      "iteration: 350, cost: 300.3242361373673\n",
      "iteration: 351, cost: 300.2996047592392\n",
      "iteration: 352, cost: 300.27552106105867\n",
      "iteration: 353, cost: 300.2519726822479\n",
      "iteration: 354, cost: 300.2289475411979\n",
      "iteration: 355, cost: 300.20643382897254\n",
      "iteration: 356, cost: 300.1844200031545\n",
      "iteration: 357, cost: 300.1628947818301\n",
      "iteration: 358, cost: 300.14184713770993\n",
      "iteration: 359, cost: 300.1212662923819\n",
      "iteration: 360, cost: 300.10114171069483\n",
      "iteration: 361, cost: 300.0814630952681\n",
      "iteration: 362, cost: 300.0622203811245\n",
      "iteration: 363, cost: 300.0434037304466\n",
      "iteration: 364, cost: 300.0250035274483\n",
      "iteration: 365, cost: 300.0070103733652\n",
      "iteration: 366, cost: 299.9894150815554\n",
      "iteration: 367, cost: 299.9722086727129\n",
      "iteration: 368, cost: 299.9553823701873\n",
      "iteration: 369, cost: 299.9389275954113\n",
      "iteration: 370, cost: 299.92283596342884\n",
      "iteration: 371, cost: 299.9070992785262\n",
      "iteration: 372, cost: 299.8917095299606\n",
      "iteration: 373, cost: 299.8766588877858\n",
      "iteration: 374, cost: 299.8619396987714\n",
      "iteration: 375, cost: 299.8475444824145\n",
      "iteration: 376, cost: 299.8334659270419\n",
      "iteration: 377, cost: 299.81969688599884\n",
      "iteration: 378, cost: 299.8062303739256\n",
      "iteration: 379, cost: 299.79305956311623\n",
      "iteration: 380, cost: 299.7801777799615\n",
      "iteration: 381, cost: 299.7675785014699\n",
      "iteration: 382, cost: 299.75525535186955\n",
      "iteration: 383, cost: 299.7432020992842\n",
      "iteration: 384, cost: 299.73141265248705\n",
      "iteration: 385, cost: 299.71988105772505\n",
      "iteration: 386, cost: 299.70860149561713\n",
      "iteration: 387, cost: 299.69756827812137\n",
      "iteration: 388, cost: 299.6867758455705\n",
      "iteration: 389, cost: 299.67621876377456\n",
      "iteration: 390, cost: 299.6658917211895\n",
      "iteration: 391, cost: 299.6557895261484\n",
      "iteration: 392, cost: 299.6459071041568\n",
      "iteration: 393, cost: 299.63623949524737\n",
      "iteration: 394, cost: 299.62678185139544\n",
      "iteration: 395, cost: 299.61752943399296\n",
      "iteration: 396, cost: 299.60847761137825\n",
      "iteration: 397, cost: 299.599621856423\n",
      "iteration: 398, cost: 299.59095774417284\n",
      "iteration: 399, cost: 299.5824809495409\n",
      "iteration: 400, cost: 299.5741872450545\n",
      "iteration: 401, cost: 299.5660724986517\n",
      "iteration: 402, cost: 299.55813267152837\n",
      "iteration: 403, cost: 299.5503638160327\n",
      "iteration: 404, cost: 299.5427620736085\n",
      "iteration: 405, cost: 299.5353236727845\n",
      "iteration: 406, cost: 299.5280449272086\n",
      "iteration: 407, cost: 299.52092223372654\n",
      "iteration: 408, cost: 299.513952070505\n",
      "iteration: 409, cost: 299.50713099519555\n",
      "iteration: 410, cost: 299.50045564314087\n",
      "iteration: 411, cost: 299.4939227256215\n",
      "iteration: 412, cost: 299.4875290281418\n",
      "iteration: 413, cost: 299.4812714087551\n",
      "iteration: 414, cost: 299.475146796426\n",
      "iteration: 415, cost: 299.4691521894297\n",
      "iteration: 416, cost: 299.4632846537883\n",
      "iteration: 417, cost: 299.45754132174187\n",
      "iteration: 418, cost: 299.4519193902532\n",
      "iteration: 419, cost: 299.4464161195476\n",
      "iteration: 420, cost: 299.4410288316851\n",
      "iteration: 421, cost: 299.43575490916487\n",
      "iteration: 422, cost: 299.43059179356095\n",
      "iteration: 423, cost: 299.4255369841892\n",
      "iteration: 424, cost: 299.4205880368038\n",
      "iteration: 425, cost: 299.4157425623241\n",
      "iteration: 426, cost: 299.41099822558886\n",
      "iteration: 427, cost: 299.40635274413995\n",
      "iteration: 428, cost: 299.4018038870323\n",
      "iteration: 429, cost: 299.3973494736716\n",
      "iteration: 430, cost: 299.39298737267825\n",
      "iteration: 431, cost: 299.3887155007758\n",
      "iteration: 432, cost: 299.38453182170605\n",
      "iteration: 433, cost: 299.3804343451676\n",
      "iteration: 434, cost: 299.37642112577845\n",
      "iteration: 435, cost: 299.37249026206274\n",
      "iteration: 436, cost: 299.3686398954592\n",
      "iteration: 437, cost: 299.3648682093527\n",
      "iteration: 438, cost: 299.3611734281284\n",
      "iteration: 439, cost: 299.357553816245\n",
      "iteration: 440, cost: 299.3540076773315\n",
      "iteration: 441, cost: 299.3505333533026\n",
      "iteration: 442, cost: 299.34712922349456\n",
      "iteration: 443, cost: 299.3437937038211\n",
      "iteration: 444, cost: 299.34052524594756\n",
      "iteration: 445, cost: 299.33732233648334\n",
      "iteration: 446, cost: 299.334183496195\n",
      "iteration: 447, cost: 299.3311072792334\n",
      "iteration: 448, cost: 299.32809227238147\n",
      "iteration: 449, cost: 299.3251370943174\n",
      "iteration: 450, cost: 299.3222403948947\n",
      "iteration: 451, cost: 299.31940085443864\n",
      "iteration: 452, cost: 299.3166171830586\n",
      "iteration: 453, cost: 299.31388811997584\n",
      "iteration: 454, cost: 299.3112124328662\n",
      "iteration: 455, cost: 299.30858891721846\n",
      "iteration: 456, cost: 299.30601639570625\n",
      "iteration: 457, cost: 299.3034937175742\n",
      "iteration: 458, cost: 299.30101975803916\n",
      "iteration: 459, cost: 299.29859341770316\n",
      "iteration: 460, cost: 299.2962136219808\n",
      "iteration: 461, cost: 299.2938793205394\n",
      "iteration: 462, cost: 299.29158948675143\n",
      "iteration: 463, cost: 299.2893431171594\n",
      "iteration: 464, cost: 299.28713923095324\n",
      "iteration: 465, cost: 299.28497686945923\n",
      "iteration: 466, cost: 299.2828550956398\n",
      "iteration: 467, cost: 299.2807729936059\n",
      "iteration: 468, cost: 299.2787296681392\n",
      "iteration: 469, cost: 299.2767242442257\n",
      "iteration: 470, cost: 299.2747558666003\n",
      "iteration: 471, cost: 299.2728236992999\n",
      "iteration: 472, cost: 299.2709269252289\n",
      "iteration: 473, cost: 299.2690647457329\n",
      "iteration: 474, cost: 299.26723638018234\n",
      "iteration: 475, cost: 299.26544106556594\n",
      "iteration: 476, cost: 299.2636780560931\n",
      "iteration: 477, cost: 299.2619466228051\n",
      "iteration: 478, cost: 299.26024605319515\n",
      "iteration: 479, cost: 299.2585756508374\n",
      "iteration: 480, cost: 299.2569347350236\n",
      "iteration: 481, cost: 299.25532264040856\n",
      "iteration: 482, cost: 299.2537387166638\n",
      "iteration: 483, cost: 299.2521823281379\n",
      "iteration: 484, cost: 299.2506528535254\n",
      "iteration: 485, cost: 299.24914968554384\n",
      "iteration: 486, cost: 299.2476722306162\n",
      "iteration: 487, cost: 299.24621990856224\n",
      "iteration: 488, cost: 299.24479215229593\n",
      "iteration: 489, cost: 299.2433884075298\n",
      "iteration: 490, cost: 299.2420081324863\n",
      "iteration: 491, cost: 299.2406507976157\n",
      "iteration: 492, cost: 299.2393158853191\n",
      "iteration: 493, cost: 299.23800288968016\n",
      "iteration: 494, cost: 299.2367113162001\n",
      "iteration: 495, cost: 299.23544068154115\n",
      "iteration: 496, cost: 299.2341905132736\n",
      "iteration: 497, cost: 299.2329603496303\n",
      "iteration: 498, cost: 299.231749739266\n",
      "iteration: 499, cost: 299.23055824102187\n",
      "iteration: 500, cost: 299.2293854236954\n",
      "iteration: 501, cost: 299.2282308658165\n",
      "iteration: 502, cost: 299.2270941554269\n",
      "iteration: 503, cost: 299.2259748898663\n",
      "iteration: 504, cost: 299.22487267556187\n",
      "iteration: 505, cost: 299.2237871278234\n",
      "iteration: 506, cost: 299.2227178706431\n",
      "iteration: 507, cost: 299.22166453649896\n",
      "iteration: 508, cost: 299.2206267661642\n",
      "iteration: 509, cost: 299.219604208519\n",
      "iteration: 510, cost: 299.2185965203684\n",
      "iteration: 511, cost: 299.21760336626295\n"
     ]
    }
   ],
   "source": [
    "betas, steps, costs = gradient_descent(X, y, alpha=0.0000003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Cnq33Adv1RD"
   },
   "source": [
    "If we plot the cost function $J$ against the number of iteration, we get a plot as shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 2780,
     "status": "ok",
     "timestamp": 1620133588136,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "dd0cgF4ADmhG",
    "outputId": "433904c2-7f3b-4e8a-e675-d1506fe8f5f3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)\n",
    "plt.xlabel(\"No. of iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.ylim(0, 2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUZ_WKyJyqev"
   },
   "source": [
    "As you can see, the cost function $J$ decreases with number of iterations and finally saturates around $299$ in $511^{th}$ iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53s4STK-wU3o"
   },
   "source": [
    "Let's see the optimal parameters found by Gradient Descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "executionInfo": {
     "elapsed": 2768,
     "status": "ok",
     "timestamp": 1620133588137,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "1vOc-SqbwhqW",
    "outputId": "87b4f55c-91b8-4ce6-a1bf-e4b83ca11feb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>TV, Radio, and Newspaper</strong> <br>\n",
       "$y$ = 1.75 + 0.05$x_{1}$ + 0.20$x_{2}$ + 0.01$x_{3}$ <br>\n",
       "$x_{1}$ = TV <br>\n",
       "$x_{2}$ = radio <br>\n",
       "$x_{3}$ = newspaper\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "message = \"\"\"<strong>TV, Radio, and Newspaper</strong> <br>\n",
    "$y$ = {:.2f} + {:.2f}$x_{{1}}$ + {:.2f}$x_{{2}}$ + {:.2f}$x_{{3}}$ <br>\n",
    "$x_{{1}}$ = TV <br>\n",
    "$x_{{2}}$ = radio <br>\n",
    "$x_{{3}}$ = newspaper\n",
    "\"\"\".format(*betas[0], *betas[1], *betas[2], *betas[3])\n",
    "display(HTML( message ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOvWRz0tGrPN"
   },
   "source": [
    "*TV, radio* and *newspaper* are the input variables, $x_1, x_2$ and $x_3$ respectively and _sales_ is the output variable, $y$. We obtain a multiple linear regression model of $y = 1.75 + 0.05x_{1} + 0.20x_{2} + 0.01x_{3}$. Note that each of the estimated model parameters (i.e., $\\beta_{0}$ through $\\beta_{3}$) have been rounded to 2 decimal places.. Intercept, $\\beta_0$ has been estimated as $1.75$ and three regression coefficients, $\\beta_1$, $\\beta_2$ and $\\beta_3$ have been estimated to $0.05$, $0.20$,  and $0.01$ respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T4RprYQ20uf"
   },
   "source": [
    "You might have noticed that the parameters found by Gradient Descent are similar but not exactly the same as that found by OLS. This is because the Gradient Descent depends on the factors such as the learning rate and the number of iterations for which it runs. Selection of these factors heavily affects the solution that the Gradient Descent finds. \n",
    "\n",
    "*You can play around with the learning rate and the number of iterations and see if the soilution matches to that of OLS* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmiCEuhOobGU"
   },
   "source": [
    "## Comparison with Ordinary Least Squares method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA81KOHEuDeX"
   },
   "source": [
    "While both of the techniques find the parameters that minimize the cost function, their approach is quite different. As mentioned earlier, Ordinary Least Squares (OLS) is a direct method to find the optimal parameters whereas the Gradient Descent is an iterative method. This is the main difference between the two approaches. Few other points that differentiate the two approaches are:\n",
    "\n",
    "* You don't need to choose any hyperparameter in OLS whereas Gradient Descent requires you to choose a learning rate and the number of iterations.\n",
    "\n",
    "* OLS requires the number of samples $n$ to be greater than the number of features $d$. Also it requires $(\\mathbf{X}^{T}\\mathbf{X})^{-1}$ to exist. However, there are no such constraints in Gradient Descent.\n",
    "\n",
    "* If the constraints of OLS are met then it always gives the exact solution. Whereas the solution provided by the Gradient descent is dependent on the learning rate and the number of iterations and may not always be exact.\n",
    "\n",
    "* The time complexity of OLS is $O(d^3)$ whereas the time complexity of Gradient Descent is $O(kd^2)$ where $k$ is the number of iterations. So if the number of features is large (exceeding $10,000$), then its a good idea to choose Gradient Descent.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJXZe1kBoJCn"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "* Gradient of the cost function w.r.t each of the parameters can be derived easily using calculus.\n",
    "\n",
    "* The parameters are updated iteratively using their corresponding gradients.\n",
    "\n",
    "* When the dataset is large with a large number of features, Gradient descent is preferred instead of OLS.\n",
    "of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Read: Gradient Descent for Linear Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
